seed: 42
exp_dir: /cognitive_comp/wutong/similarity_generation/experiments/
txl_config_path: /cognitive_comp/wutong/similarity_generation/model_utils/txl_5B_config.json

txl_model_path: /cognitive_comp/wutong/source/model_base/txl_zh_5.0B.pt
dis_model_path: /cognitive_comp/wutong/source/model_base
sp_model_path: /cognitive_comp/wutong/source/model_base/chinese_sentencepiece/cog-pretrain.model

test_sentence_path: /cognitive_comp/wutong/source/sim_data/predict_sentences/
test_data_path: /cognitive_comp/wutong/source/sim_data/sim_test_data/
lab_data_path: /cognitive_comp/wutong/source/sim_data/sim_train_data/

# 需要修改文件名
ckpt_model_path: /cognitive_comp/wutong/similarity_generation/experiments/lightning_logs/checkpoints  ## 
sim_data_path: /cognitive_comp/wutong/source/exp_data/sim_cycle_data  ##
cache_data_path: /cognitive_comp/wutong/source/exp_data/sim_cycle_cache  ##

top_k: 0
top_p: 0.95
repetition_penalty: 1.0

max_thre0: 0.5
min_thre0: 0.5
max_thre1: 0.9
min_thre1: 0.6
max_dis_thre: 0.8
min_dis_thre: 0.6

gen_repeat_times: 1  # 每条句子重复生成的次数
dis_batch_size: 128  # 128 / 384
gen_batch_size: 2
gen_big_batch_size: 30
pre_gen_bs: 512  # 生成样本的batch size
pre_dis_bs: 1024

warm_up_model: False  ####
pretrain_dis: True  ####
dis_hidden_size: 1024
discriminator: hfl/chinese-roberta-wwm-ext-large ##
# hfl/chinese-roberta-wwm-ext-large / IDEA-CCNL/Erlangshen-Roberta-330M-Similarity

learning_rate: 2e-5
cycle_num: 1  ####
cycle: 0  ####
data_num: -1

gen_train_steps: 200
dis_train_steps: 5000  ####
warmup_steps: 500  ####
checkpoint_steps: 10
es_patience: 1
data_name: afqmc  # chip / paws / afqmc / qqp  ##
