seed: 42
exp_dir: /cognitive_comp/wutong/similarity_generation/experiments/
txl_config_path: /cognitive_comp/wutong/similarity_generation/model_utils/txl_5B_config.json

model_path: /cognitive_comp/wutong/source/model_base/model_en/  # zh / en
sp_model_path: /cognitive_comp/wutong/source/model_base/chinese_sentencepiece/cog-pretrain.model

test_sentence_path: /cognitive_comp/wutong/source/sim_data/predict_sentences/
test_data_path: /cognitive_comp/wutong/source/sim_data/sim_test_data/
lab_data_path: /cognitive_comp/wutong/source/sim_data/sim_train_data/

ckpt_model_path: /cognitive_comp/wutong/similarity_generation/experiments/lightning_logs/checkpoints/
sim_data_path: /cognitive_comp/wutong/source/exp_data/sim_cycle_data/
cache_data_path: /cognitive_comp/wutong/source/exp_data/sim_cycle_cache/

top_k: 0
top_p: 0.8
std_scale: 1.2
repetition_penalty: 1.0

gen_nums: 1
gen_repeat_times: 1  # 每条句子重复生成的次数
dis_batch_size: 32  # 64 / 128
gen_batch_size: 2
gen_big_batch_size: 30
gen_en_batch_size: 4
pre_gen_bs: 32  # 生成样本的batch size
pre_dis_bs: 1024

dis_balance: True
dis_use_label: True
warm_up_model: True  ####
pretrain_gen: False  ####
pretrain_dis: False  ####
chinese: False

dis_hidden_size: 1024
discriminator: albert_xxlarge  ## roberta_large
# hfl/chinese-roberta-wwm-ext-large / IDEA-CCNL/Erlangshen-Roberta-330M-Similarity

learning_rate: 2e-5
cycle_num: 10  ####
cycle: -1  ####
idx: -1
start: 0 
end: 0
sentence_num: 5000

gen_train_steps: 200
dis_train_steps: 200  ####
warmup_steps: 20  ####
es_patience: 1

vae2gen: False
txl2gen: True
