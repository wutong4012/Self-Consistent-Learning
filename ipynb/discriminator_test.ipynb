{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AlbertTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def discriminator_collate_fn(batch_data, tokenizer):\n",
    "    dis_text_input_ids, labels = [], []\n",
    "    for item in batch_data:\n",
    "        # sentence1\n",
    "        dis_text = item['text1'] + '[SEP]' + item['text2']\n",
    "        input_ids = tokenizer(dis_text, return_tensors='pt').input_ids.squeeze()\n",
    "\n",
    "        dis_text_input_ids.append(input_ids)\n",
    "        labels.append(torch.tensor(int(item['score']), dtype=torch.long))\n",
    "\n",
    "    dis_text_input_ids = pad_sequence([x for x in dis_text_input_ids],\n",
    "                                      batch_first=True, \n",
    "                                      padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    return {\n",
    "        'dis_text_input_ids': dis_text_input_ids,\n",
    "        'labels': torch.stack(labels),\n",
    "    }\n",
    "\n",
    "\n",
    "class Config:\n",
    "    cycle = 0\n",
    "    zero_shot = 0  #\n",
    "    data_name = 'mrpc'  #\n",
    "    chinese = 0 # \n",
    "    warm_up_model = True  #\n",
    "    pretrain_dis = False\n",
    "    discriminator_en = 'albert_xxlarge'\n",
    "    discriminator_zh = 'albert_xxlarge'  # roformer_large / roberta_large\n",
    "    pretrained_en = '/cognitive_comp/wutong/source/model_base/pretrained_en/'\n",
    "    pretrained_zh = '/cognitive_comp/wutong/source/model_base/pretrained_zh/'\n",
    "    ckpt_model_path = '/cognitive_comp/wutong/similarity_generation/experiments/lightning_logs/checkpoints/2'\n",
    "    # ckpt_model_path = '/cognitive_comp/wutong/similarity_generation/all_checkpoints/new_exp7'\n",
    "\n",
    "class SimGanDataset(Dataset):\n",
    "    def __init__(self, data) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import sys, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sys.path.append('/cognitive_comp/wutong/similarity_generation/')\n",
    "from model_utils.sim_gen_model import Discriminator\n",
    "\n",
    "\n",
    "config = Config()\n",
    "data = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/sim_test_data/' + config.data_name)\n",
    "dataset = SimGanDataset(data)\n",
    "dis_tokenizer = AlbertTokenizer.from_pretrained(config.pretrained_en + config.discriminator_en)\n",
    "# dis_tokenizer = AutoTokenizer.from_pretrained(config.pretrained_zh + config.discriminator_zh)\n",
    "def collate_fn(batch_data):\n",
    "    return discriminator_collate_fn(batch_data, dis_tokenizer)\n",
    "dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=512,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "pred_result = []\n",
    "f1_result, acc_result = [], []\n",
    "for idx in range(11):\n",
    "    config.cycle = idx\n",
    "    discriminator = Discriminator(config)\n",
    "    discriminator.cuda().eval()\n",
    "    with torch.no_grad():\n",
    "        pred_list = []\n",
    "        f1_score_list, acc_score_list = [], []\n",
    "        for batch in dataloader:\n",
    "            torch.cuda.empty_cache()\n",
    "            logits = discriminator.forward(\n",
    "                batch['dis_text_input_ids'].cuda(),\n",
    "                None\n",
    "            )\n",
    "            \n",
    "            predictions = torch.argmax(logits, dim=1).tolist()\n",
    "            f1_score_list.append(\n",
    "                f1_score(batch['labels'].cuda().tolist(), predictions)\n",
    "            )\n",
    "            acc_score_list.append(\n",
    "                accuracy_score(batch['labels'].cuda().tolist(), predictions)\n",
    "            )\n",
    "        print(sum(f1_score_list) / len(f1_score_list))\n",
    "        f1_result.append(sum(f1_score_list) / len(f1_score_list))\n",
    "        print(sum(acc_score_list) / len(acc_score_list))\n",
    "        acc_result.append(sum(acc_score_list) / len(acc_score_list))\n",
    "\n",
    "print(f1_result)\n",
    "print(acc_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import sys\n",
    "sys.path.append('/cognitive_comp/wutong/similarity_generation/')\n",
    "from data_utlis.sim_data_collate import padding_dis_mask\n",
    "\n",
    "\n",
    "def dis_pred_collate(batch_data, tokenizer):\n",
    "    max_length = 0\n",
    "    input_ids, token_type_ids, attention_mask, position_ids = [], [], [], []\n",
    "    clslabels_mask, sentence1, sentence2, labels, label_idx = [], [], [], [], []\n",
    "    for item in batch_data:\n",
    "        max_length = max(max_length, item['attention_mask'].size(0))\n",
    "        input_ids.append(item['input_ids'])\n",
    "        token_type_ids.append(item['token_type_ids'])\n",
    "        attention_mask.append(item['attention_mask'])\n",
    "        position_ids.append(item['position_ids'])\n",
    "        clslabels_mask.append(item['clslabels_mask'])\n",
    "        sentence1.append(item['sentence1'])\n",
    "        sentence2.append(item['sentence2'])\n",
    "        labels.append(item['label'])\n",
    "        label_idx.append(item['label_idx'])\n",
    "    \n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    token_type_ids = pad_sequence(token_type_ids, batch_first=True, padding_value=0)\n",
    "    attention_mask = padding_dis_mask(attention_mask, max_length)\n",
    "    position_ids = pad_sequence(position_ids, batch_first=True, padding_value=0)\n",
    "    clslabels_mask = pad_sequence(clslabels_mask, batch_first=True, padding_value=-10000)\n",
    "        \n",
    "    return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"position_ids\": position_ids,\n",
    "            \"clslabels_mask\": clslabels_mask,\n",
    "            'label_idx': torch.stack(label_idx),\n",
    "            'sentence1': sentence1,\n",
    "            'sentence2': sentence2,\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "import datasets, torch\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from data_utlis.sim_gen_dataset import SimGanDataset, preprocess\n",
    "from model_utils.sim_gen_model import Discriminator\n",
    "\n",
    "\n",
    "class Config:\n",
    "    cycle = 0\n",
    "    warm_up_model = True\n",
    "    data_path = '/cognitive_comp/wutong/source/sim_data/raw_data/bustm'\n",
    "    dis_model_path = '/cognitive_comp/wutong/source/model_base/pretrained_zh/macbert_large_mc'\n",
    "    dis_ckpt_path = '/cognitive_comp/wutong/finetune_large.bin'\n",
    "    ckpt_model_path = '/cognitive_comp/wutong/similarity_generation/experiments/lightning_logs/checkpoints'\n",
    "    \n",
    "config = Config()\n",
    "dis_tokenizer = AutoTokenizer.from_pretrained(config.dis_model_path)\n",
    "\n",
    "test_data = datasets.Dataset.from_json(config.data_path + '/test_public.json')\n",
    "test_data = test_data.map(preprocess)\n",
    "test_dataset = SimGanDataset(data=test_data, tokenizer=dis_tokenizer, test=True)\n",
    "def collate_fn(batch_data):\n",
    "    return dis_pred_collate(batch_data, dis_tokenizer)\n",
    "dataloader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=512,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "pred_result = []\n",
    "f1_result, acc_result = [], []\n",
    "all_labels, all_preds = [], []\n",
    "for idx in range(4):\n",
    "    config.cycle = idx\n",
    "    discriminator = Discriminator(config, dis_tokenizer)\n",
    "    discriminator.cuda().eval()\n",
    "    with torch.no_grad():\n",
    "        pred_list = []\n",
    "        f1_score_list, acc_score_list = [], []\n",
    "        for batch in dataloader:\n",
    "            all_logits = []\n",
    "            torch.cuda.empty_cache()\n",
    "            prob = discriminator.forward(\n",
    "                input_ids=batch['input_ids'].cuda(),\n",
    "                attention_mask=batch['attention_mask'].cuda(),\n",
    "                token_type_ids=batch['token_type_ids'].cuda(),\n",
    "                position_ids=batch['position_ids'].cuda(),\n",
    "                clslabels_mask=batch['clslabels_mask'].cuda(),\n",
    "                bt_label_idx=batch['label_idx'].cuda()\n",
    "            )\n",
    "            \n",
    "            predictions = torch.argmax(prob, dim=-1).tolist()\n",
    "            all_labels.extend(batch['labels'])\n",
    "            all_preds.extend(predictions)\n",
    "            \n",
    "        print(f1_score(all_labels, all_preds))\n",
    "        f1_result.append(f1_score(all_labels, all_preds))\n",
    "        print(accuracy_score(all_labels, all_preds))\n",
    "        acc_result.append(accuracy_score(all_labels, all_preds))\n",
    "\n",
    "print(f1_result)\n",
    "print(acc_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roberta = paws:[0.8997369931329687] [0.903875942887931]\n",
    "#          mrpc = [0.7673407401588641] [0.6276403356481481]\n",
    "# xlnet = paws:[0.47952997670125214] [0.5578023976293103] //es=3都不行\n",
    "#        mrpc = [0.7941365210413075] [0.659014343584656]\n",
    "# bert = paws:[0.9213888861666291] [0.9253771551724138]\n",
    "#       mrpc = [0.8156460192358714] [0.7269112723214286]\n",
    "# electra = paws:[0.9214410094402226] [0.9246868265086207]\n",
    "#          mrpc = [0.7939015098372849] [0.658526062334656]\n",
    "# xlm-roberta = paws:[0.905323910226371] [0.9117894665948276]\n",
    "#              mrpc = [0.8426940116719521] [0.7911861359126984]\n",
    "# albert = paws:[0.9604555945805979] [0.963934536637931]\n",
    "#         mrpc = [0.8893473320445198] [0.8592251570767195]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# electra = afqmc:[0.545895310635966] [0.7228772095959596]\n",
    "#          chip = [0.849673048762821] [0.8430739182692307]\n",
    "#          qqp = [0.7180762308328926] [0.781982421875]\n",
    "# macbert = afmqc:[0.6111182068285741] [0.7516611426767676]\n",
    "#          chip:[0.8593980022419215] [0.8554875300480769]\n",
    "#          qqp = [0.7294041926803511] [0.781982421875]\n",
    "# rofomer = afmqc:[0.6418719522930292] [0.7750946969696969]\n",
    "#          chip:[0.8415617903407737] [0.8429987980769231]\n",
    "#          qqp = [0.765667422645637] [0.813232421875]\n",
    "# structbert = afmqc:[0.6056075382357595] [0.7497001262626263]\n",
    "#             chip:[0.8516500083224742] [0.8484825721153846]\n",
    "#             qqp:[0.7732818709313707] [0.813720703125]\n",
    "# xlnet = afmqc:[0.5031211435150875] [0.6857796717171717]\n",
    "#        chip:[0.8296630108013876] [0.8177396334134616]\n",
    "#        qqp:[0.6496849405633153] [0.73095703125]\n",
    "# albert = afmqc:[0.5686514435362098] [0.7508680555555556]\n",
    "#         chip:[0.8632537912808637] [0.8587552584134616]\n",
    "#         qqp:[0.6600652114652226] [0.747802734375]\n",
    "# roberta = afmqc:[0.5729197585634695] [0.7502130681818182]\n",
    "#          chip:[0.8692697270032463] [0.8669057992788461]\n",
    "#          qqp:[0.7458100776127146] [0.7978515625]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "y = [0.3450724267046834, 0.43526217574723775, 0.33945017097133845, 0.3619290824702441, 0.36383696270018284, \n",
    "0.39280227933930073, 0.36088015890524094, 0.3230967660316284, 0.2617301088737617, 0, 0.18178095085881757, 0.0892337549256072, 0.07488227572939889, 0.07151414141036311, 0.07504186251877931, \n",
    "0.05583403570053498, 0.05289366441930157, 0.03416507607136132, 0.033453614692229725, 0.024714469986622287, \n",
    "0.02888675736874282, 0, 0.2189011242828891, 0.14067707286676562, 0.2031573829354798, 0.12241785405091402, 0.13518911602506856, \n",
    "0.12585420520606475, 0.0899325954908943, 0.12480431998217825, 0.12156096303645424, 0.090277737478651, \n",
    "0.050300214886722844, 0, 0.3523108767950005, 0.11316448387455255, 0.030861984236866754, 0.021433970858802387, 0.011469380308480093, \n",
    "0.012914158734129873, 0.016765391879763763, 0.012811650941711477, 0.02138641281263294]\n",
    "test = pd.DataFrame(data=y)\n",
    "print(test)\n",
    "test.to_csv('test.csv',encoding='gbk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding=utf-8\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "fig, axes = plt.subplots(ncols=4, figsize=(28,7))  # , nrows=2\n",
    "[ax1, ax2, ax3, ax4] = axes\n",
    " \n",
    "x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
    "selective_trust = np.array([0.38254154474255286, 0.3960984181095649, 0.44978116229809234, 0.45989012915572847, 0.45709906383664045, \n",
    "0.4801106211003655, 0.504149500468291, 0.506782448928266, 0.5136386072545487])\n",
    "fullly_trust = np.array([0.38254154474255286, 0.46877269447755715, 0.46909745565333594, 0.4692954930270959, 0.46966238165481183, \n",
    "0.46911981674991676, 0.4690417776913679, 0.47011498891802805, 0.4687900941179161])\n",
    "no_label = np.array([0.38254154474255286, 0.4064452662911879, 0.44236206320170124, 0.47367297728223673, 0.4850168321537159, \n",
    "0.5012746902756866, 0.5037796076160921, 0.5026058825825503, 0.5021541312154869])\n",
    " \n",
    "ax1.plot(x, selective_trust, label='Proposed Method', color='red', linestyle='-', marker='o', markerfacecolor='black', markersize='10')\n",
    "ax1.plot(x, fullly_trust, label='No Selection Mechanism', color='blue', linestyle=':', marker='*', markerfacecolor='black', markersize='10')\n",
    "ax1.plot(x, no_label, label='No Pretraining Data', color='green', linestyle='--', marker='^', markerfacecolor='black', markersize='10')\n",
    "\n",
    "#设置坐标轴\n",
    "ax1.set_title('AFQMC', color='black', fontsize=25)\n",
    "ax1.set_xlabel('Round Nums', color='black', fontsize=15)\n",
    "ax1.set_ylabel('F1 Score', color='black', fontsize=15)\n",
    "ax1.tick_params(axis='both', labelcolor='black', labelsize=15, width=3, color='black')\n",
    "\n",
    "#显示网格\n",
    "#ax.grid(True, linestyle='-.')\n",
    "ax1.yaxis.grid(True, linestyle='-.')\n",
    "#添加图例\n",
    "legend = ax1.legend(loc='best', fontsize=15)\n",
    "\n",
    "\n",
    "\n",
    "x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "selective_trust = np.array([0.5881660637636877, 0.6289374431681133, 0.6723658508141568, 0.713794292460992, 0.7145139426994398, \n",
    "0.740641428621894, 0.7408486799912206, 0.7665794132256334, 0.7630026406183327, 0.7666930433539052, \n",
    "0.774214486289817])\n",
    "fullly_trust = np.array([0.5881660637636877, 0.6636617726976335, 0.6642206660203174, 0.6639590979377188, 0.6641740286236856, \n",
    "0.6642811330314553, 0.6640371291044285, 0.6638241591952845, 0.6638179210213015, 0.6642828177708464, \n",
    "0.6640151542979859])\n",
    "no_label = np.array([0.5881660637636877, 0.6358490369533671, 0.6480864578022787, 0.651020791966919, 0.6564546334248053, \n",
    "0.6734860331918114, 0.6902199014502423, 0.6888101552952934, 0.688709687421092, 0.6940691606531408,\n",
    "0.6962635170769439])\n",
    " \n",
    "ax2.plot(x, selective_trust, label='Proposed Method', color='red', linestyle='-', marker='o', markerfacecolor='black', markersize='10')\n",
    "ax2.plot(x, fullly_trust, label='No Selection Mechanism', color='blue', linestyle=':', marker='*', markerfacecolor='black', markersize='10')\n",
    "ax2.plot(x, no_label, label='No Pretraining Data', color='green', linestyle='--', marker='^', markerfacecolor='black', markersize='10')\n",
    "\n",
    "#设置坐标轴\n",
    "ax2.set_title('CHIP-STS', color='black', fontsize=25)\n",
    "ax2.set_xlabel('Round Nums', color='black', fontsize=15)\n",
    "ax2.set_ylabel('F1 Score', color='black', fontsize=15)\n",
    "ax2.tick_params(axis='both', labelcolor='black', labelsize=15, width=3, color='black')\n",
    "\n",
    "#显示网格\n",
    "#ax.grid(True, linestyle='-.')\n",
    "ax2.yaxis.grid(True, linestyle='-.')\n",
    "#添加图例\n",
    "legend = ax2.legend(loc='best', fontsize=15)\n",
    "\n",
    "\n",
    "\n",
    "x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "selective_trust = np.array([0.5787733523556429, 0.600761073931724, 0.5857334315782603, 0.6030196371638206, 0.613082687657, \n",
    "0.6446848063163415, 0.664392721223111, 0.6387986670145489, 0.6545834340943931, 0.6807849710717174, \n",
    "0.7050992611635148])\n",
    "fullly_trust = np.array([0.5787733523556429, 0.6374456960733875, 0.6312069203979489, 0.6361256846512542, 0.6368882523259844, \n",
    "0.6334402436786893, 0.6385384809511449, 0.634539659014934, 0.632161278274643, 0.6362743578965781, \n",
    "0.6344880601932359])\n",
    "no_label = np.array([0.5787733523556429, 0.5904298590861632, 0.6394423836030231, 0.6477267406003373, 0.6715233623036275, \n",
    "0.66688813607648, 0.6831579329962527, 0.6851558182653499, 0.6811772008040317, 0.676038318180333, \n",
    "0.6885035291405497])\n",
    " \n",
    "ax3.plot(x, selective_trust, label='Proposed Method', color='red', linestyle='-', marker='o', markerfacecolor='black', markersize='10')\n",
    "ax3.plot(x, fullly_trust, label='No Selection Mechanism', color='blue', linestyle=':', marker='*', markerfacecolor='black', markersize='10')\n",
    "ax3.plot(x, no_label, label='No Pretraining Data', color='green', linestyle='--', marker='^', markerfacecolor='black', markersize='10')\n",
    "\n",
    "#设置坐标轴\n",
    "ax3.set_title('Chinese-QQP', color='black', fontsize=25)\n",
    "ax3.set_xlabel('Round Nums', color='black', fontsize=15)\n",
    "ax3.set_ylabel('F1 Score', color='black', fontsize=15)\n",
    "ax3.tick_params(axis='both', labelcolor='black', labelsize=15, width=3, color='black')\n",
    "\n",
    "#显示网格\n",
    "#ax.grid(True, linestyle='-.')\n",
    "ax3.yaxis.grid(True, linestyle='-.')\n",
    "#添加图例\n",
    "legend = ax3.legend(loc='best', fontsize=15)\n",
    "\n",
    "\n",
    "\n",
    "x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
    "selective_trust = np.array([0.685425149429463, 0.7547010093663151, 0.7662849329866745, 0.8300382835835954, 0.8389952050899818, \n",
    "0.8424136200685073, 0.8450697319911847, 0.8432245404107888, 0.8461271960949839])\n",
    "fullly_trust = np.array([0.685425149429463, 0.7955473204511441, 0.7939015098372849, 0.7939015098372849, 0.7939015098372849, \n",
    "0.7941354097281316, 0.7941354097281316, 0.7941354097281316, 0.7941354097281316])\n",
    "no_label = np.array([0.685425149429463, 0.6818982417546661, 0.7375619526379604, 0.8209849067032067, 0.834055656344813, \n",
    "0.8345211380648245, 0.8368184565131172, 0.8292048620492494, 0.8321404582284176])\n",
    " \n",
    "ax4.plot(x, selective_trust, label='Proposed Method', color='red', linestyle='-', marker='o', markerfacecolor='black', markersize='10')\n",
    "ax4.plot(x, fullly_trust, label='No Selection Mechanism', color='blue', linestyle=':', marker='*', markerfacecolor='black', markersize='10')\n",
    "ax4.plot(x, no_label, label='No Pretraining Data', color='green', linestyle='--', marker='^', markerfacecolor='black', markersize='10')\n",
    "\n",
    "#设置坐标轴\n",
    "ax4.set_title('MRPC', color='black', fontsize=25)\n",
    "ax4.set_xlabel('Round Nums', color='black', fontsize=15)\n",
    "ax4.set_ylabel('F1 Score', color='black', fontsize=15)\n",
    "ax4.tick_params(axis='both', labelcolor='black', labelsize=15, width=3, color='black')\n",
    "\n",
    "#显示网格\n",
    "# ax1.grid(True, linestyle='-.')\n",
    "# ax2.grid(True, linestyle='-.')\n",
    "# ax3.grid(True, linestyle='-.')\n",
    "# ax4.grid(True, linestyle='-.')\n",
    "ax4.yaxis.grid(True, linestyle='-.')\n",
    "#添加图例\n",
    "legend = ax4.legend(loc='best', fontsize=15)\n",
    " \n",
    "plt.savefig('./filename.svg', format='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    " \n",
    "x1 = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
    "x2 = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "afqmc = np.array([0.28654564444219904, 0.35972071429313923, 0.29298804289042746, 0.3306807832380508, 0.3324986684131944, \n",
    "0.3645114140726299, 0.3424742080541955, 0.3140620108209938, 0.2628001582167687])\n",
    "chip = np.array([0.16206089395187703, 0.09569190730008093, 0.09556990179221979, 0.09420757335900724, 0.10161252788187716, \n",
    "0.09115370391518327, 0.08399135543848252, 0.07265377248154187, 0.07835081026023685, 0.0701899316398505, 0.0809814560033198])\n",
    "qqp = np.array([0.18226184063713147, 0.121416109122202, 0.17365331953532936, 0.10583140891885817, 0.11715967163923266, \n",
    "0.11437480329256752, 0.08495643617900697, 0.11638218546801168, 0.11427012829847398, 0.08989371306790174, 0.05884738531972761])\n",
    "mrpc = np.array([0.22290342796420745, 0.07787502813333516, 0.02161599728930512, 0.01768073187058273, 0.011070382003902497, \n",
    "                 0.0153173071317978, 0.01596686939883061, 0.014495716325335358, 0.021032058399025413])\n",
    " \n",
    "ax.plot(x1, afqmc, label='AFQMC', color='orange', linestyle='-.', marker='s', markerfacecolor='black', markersize='10')\n",
    "ax.plot(x2, chip, label='CHIP-STS', color='blue', linestyle=':', marker='*', markerfacecolor='black', markersize='10')\n",
    "ax.plot(x2, qqp, label='Chinese-QQP', color='green', linestyle='--', marker='^', markerfacecolor='black', markersize='10')\n",
    "ax.plot(x1, mrpc, label='MRPC', color='red', linestyle='-', marker='o', markerfacecolor='black', markersize='10')\n",
    "\n",
    "#设置坐标轴\n",
    "ax.set_xlabel('Round Nums', color='black', fontsize=20)\n",
    "ax.set_ylabel('KL Divergence', color='black', fontsize=20)\n",
    "ax.tick_params(axis='both', labelcolor='black', labelsize=20, width=3, color='black')\n",
    "\n",
    "#显示网格\n",
    "# ax.grid(True, linestyle='-.')\n",
    "ax.yaxis.grid(True, linestyle='-.')\n",
    "#添加图例\n",
    "legend = ax.legend(loc='best', fontsize=20)\n",
    "\n",
    "plt.savefig('./filename.svg', format='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 处理数据集(json->datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 处理AFQMC数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "path = '/cognitive_comp/wutong/source/sim_data/raw_data/AFQMC/afqmc_test.json'\n",
    "feats = datasets.Features({\"text1\": datasets.Value('string'), \n",
    "                           \"text2\": datasets.Value('string'),\n",
    "                        #    \"score\": datasets.Value('int8'),\n",
    "                           \"id\": datasets.Value('int64'),\n",
    "                           })\n",
    "ds = (datasets.load_dataset('json', data_files=path, \n",
    "                            cache_dir='/cognitive_comp/wutong/source/data_base/huggingface-cache',\n",
    "                            features=feats)['train'])\n",
    "ds.save_to_disk('/cognitive_comp/wutong/source/sim_data/similarity_data/afqmc_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "label_ds = datasets.load_from_disk('/cognitive_comp/wutong/source/data_base/similarity_data/labeled_data')\n",
    "afqmc_ds = datasets.load_from_disk('/cognitive_comp/wutong/source/data_base/similarity_data/afqmc_train')\n",
    "label_afqmc_ds = datasets.concatenate_datasets([label_ds, afqmc_ds])\n",
    "label_afqmc_ds.save_to_disk('/cognitive_comp/wutong/source/data_base/similarity_data/labeled_afqmc_ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets, json\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_afqmc = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/sim_train_data/afqmc_train_ds')\n",
    "# dev_afqmc = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/sim_test_data/afqmc')\n",
    "test_afqmc = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/similarity_data/afqmc_test')\n",
    "afqmc = datasets.concatenate_datasets([train_afqmc, test_afqmc])\n",
    "afqmc = afqmc.shuffle(seed=42)\n",
    "\n",
    "with open('/cognitive_comp/wutong/source/sim_data/similarity_data/afqmc_train_ds.json', 'w') as wp:\n",
    "    for idx in tqdm(range(afqmc.num_rows)):\n",
    "        wp.write(json.dumps({'sentence': afqmc[idx]['text1']}, ensure_ascii=False) + '\\n')\n",
    "        wp.write(json.dumps({'sentence': afqmc[idx]['text2']}, ensure_ascii=False) + '\\n')\n",
    "        wp.flush()\n",
    "wp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = (datasets.load_dataset('json', data_files='/cognitive_comp/wutong/source/sim_data/similarity_data/afqmc_train_ds.json',\n",
    "                            cache_dir='/cognitive_comp/wutong/source/data_base/huggingface-cache')['train'])\n",
    "ds.save_to_disk('/cognitive_comp/wutong/source/sim_data/predict_sentences/afqmc_sentence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 处理QQP数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets, glob\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "feats = datasets.Features({\"text1\": datasets.Value('string'), \n",
    "                           \"text2\": datasets.Value('string'),\n",
    "                           \"score\": datasets.Value('int8')})\n",
    "def _generate_cache_arrow(index, path):\n",
    "    print('saving dataset shard {}'.format(index))\n",
    "    ds = (datasets.load_dataset('json', data_files=path,\n",
    "                                cache_dir='/cognitive_comp/wutong/source/data_base/huggingface-cache',\n",
    "                                features=feats)['train'])\n",
    "    ds.save_to_disk(os.path.join('/cognitive_comp/wutong/source/sim_data/translate_data/translate_cache_data', f'0{index}'))\n",
    "    return 'saving dataset shard {} done'.format(index)\n",
    "\n",
    "\n",
    "def generate_cache_arrow(num_proc=1) -> None:\n",
    "    data_dict_paths = []\n",
    "    data_dict_paths = glob.glob('/cognitive_comp/wutong/source/sim_data/translate_data/translate_json_data/*')\n",
    "    print(data_dict_paths)\n",
    "    \n",
    "    p = ProcessPoolExecutor(max_workers=num_proc)\n",
    "    res = []\n",
    "\n",
    "    for index, path in enumerate(data_dict_paths):\n",
    "        res.append(p.submit(_generate_cache_arrow, index, path))\n",
    "\n",
    "    p.shutdown(wait=True)\n",
    "    for future in res:\n",
    "        print(future.result(), flush=True)\n",
    "\n",
    "generate_cache_arrow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "cache_dict_paths = glob.glob('/cognitive_comp/wutong/source/sim_data/translate_data/translate_cache_data/*')\n",
    "sim_ds_list = []\n",
    "for path in cache_dict_paths:\n",
    "    sim_ds_list.append(datasets.load_from_disk(path))\n",
    "sim_dataset = datasets.concatenate_datasets(sim_ds_list)\n",
    "sim_dataset.save_to_disk('/cognitive_comp/wutong/source/sim_data/translate_data/qqp_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dataset = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/translate_data/qqp_data')\n",
    "split_ds = sim_dataset.train_test_split(test_size=0.4)\n",
    "split_ds['train'].save_to_disk('/cognitive_comp/wutong/source/sim_data/sim_train_data/qqp_train_ds')\n",
    "split_ds['test'].save_to_disk('/cognitive_comp/wutong/source/sim_data/sim_test_data/qqp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets, json\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_qqp = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/sim_train_data/qqp_train_ds')\n",
    "with open('/cognitive_comp/wutong/source/sim_data/similarity_data/qqp_train_data.json', 'w') as wp:\n",
    "    for idx in tqdm(range(train_qqp.num_rows)):\n",
    "        wp.write(json.dumps({'sentence': train_qqp[idx]['text1']}, ensure_ascii=False) + '\\n')\n",
    "        wp.write(json.dumps({'sentence': train_qqp[idx]['text2']}, ensure_ascii=False) + '\\n')\n",
    "        wp.flush()\n",
    "wp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = (datasets.load_dataset('json', data_files='/cognitive_comp/wutong/source/sim_data/similarity_data/qqp_train_data.json',\n",
    "                            cache_dir='/cognitive_comp/wutong/source/data_base/huggingface-cache')['train'])\n",
    "ds.save_to_disk(os.path.join('/cognitive_comp/wutong/source/sim_data/predict_sentences/qqp_sentence'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 处理CHIP数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "with open('/cognitive_comp/wutong/source/sim_data/raw_data/test.json', 'r') as rp:\n",
    "    data = json.load(rp)\n",
    "rp.close()\n",
    "\n",
    "with open('/cognitive_comp/wutong/source/sim_data/raw_data/chip_test.json', 'w') as wp:\n",
    "    for item in tqdm(data):\n",
    "        wp.write(json.dumps({'text1': item['text1'],\n",
    "                             'text2': item['text2'],\n",
    "                            #  'score': item['label']\n",
    "                             },\n",
    "                            ensure_ascii=False) + '\\n')\n",
    "wp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "path = '/cognitive_comp/wutong/source/sim_data/raw_data/CHIP/chip_test.json'\n",
    "feats = datasets.Features({\"text1\": datasets.Value('string'), \n",
    "                           \"text2\": datasets.Value('string'),\n",
    "                        #    \"score\": datasets.Value('int8')\n",
    "                           })\n",
    "ds = (datasets.load_dataset('json', data_files=path, \n",
    "                            cache_dir='/cognitive_comp/wutong/source/data_base/huggingface-cache',\n",
    "                            features=feats)['train'])\n",
    "ds.save_to_disk('/cognitive_comp/wutong/source/sim_data/similarity_data/chip_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets, json\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_chip = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/sim_train_data/chip_train_ds')\n",
    "# dev_chip = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/sim_test_data/chip')\n",
    "test_chip = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/similarity_data/chip_test')\n",
    "chip = datasets.concatenate_datasets([train_chip, test_chip])\n",
    "chip = chip.shuffle(seed=42)\n",
    "\n",
    "with open('/cognitive_comp/wutong/source/sim_data/similarity_data/chip_train_ds.json', 'w') as wp:\n",
    "    for idx in tqdm(range(chip.num_rows)):\n",
    "        wp.write(json.dumps({'sentence': chip[idx]['text1']}, ensure_ascii=False) + '\\n')\n",
    "        wp.write(json.dumps({'sentence': chip[idx]['text2']}, ensure_ascii=False) + '\\n')\n",
    "        wp.flush()\n",
    "wp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = (datasets.load_dataset('json', data_files='/cognitive_comp/wutong/source/sim_data/similarity_data/chip_train_ds.json',\n",
    "                            cache_dir='/cognitive_comp/wutong/source/data_base/huggingface-cache')['train'])\n",
    "ds.save_to_disk('/cognitive_comp/wutong/source/sim_data/predict_sentences/chip_sentence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 处理OPPO数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# test, train, dev: 50000 167173 10000\n",
    "with open('/cognitive_comp/wutong/source/sim_data/raw_data/oppp.json', 'r') as rp:\n",
    "    data = json.load(rp)\n",
    "rp.close()\n",
    "\n",
    "with open('/cognitive_comp/wutong/source/sim_data/raw_data/oppo_train.json', 'w') as wp:\n",
    "    for item in tqdm(data['train']):\n",
    "        wp.write(json.dumps({'text1': item['q1'],\n",
    "                             'text2': item['q2'],\n",
    "                             'score': item['label'],\n",
    "                             }, ensure_ascii=False) + '\\n')\n",
    "wp.close()\n",
    "\n",
    "with open('/cognitive_comp/wutong/source/sim_data/raw_data/oppo_dev.json', 'w') as wp1:\n",
    "    for item in tqdm(data['dev']):\n",
    "        wp1.write(json.dumps({'text1': item['q1'],\n",
    "                             'text2': item['q2'],\n",
    "                             'score': item['label'],\n",
    "                             }, ensure_ascii=False) + '\\n')\n",
    "wp1.close()\n",
    "\n",
    "with open('/cognitive_comp/wutong/source/sim_data/raw_data/oppo_test.json', 'w') as wp2:\n",
    "    for item in tqdm(data['test']):\n",
    "        wp2.write(json.dumps({'text1': item['q1'],\n",
    "                             'text2': item['q2'],\n",
    "                             }, ensure_ascii=False) + '\\n')\n",
    "wp2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "path = '/cognitive_comp/wutong/source/sim_data/raw_data/oppo_test.json'\n",
    "feats = datasets.Features({\"text1\": datasets.Value('string'), \n",
    "                           \"text2\": datasets.Value('string'),\n",
    "                        #    \"score\": datasets.Value('int8')\n",
    "                           })\n",
    "ds = (datasets.load_dataset('json', data_files=path, \n",
    "                            cache_dir='/cognitive_comp/wutong/source/data_base/huggingface-cache',\n",
    "                            features=feats)['train'])\n",
    "# ds.save_to_disk('/cognitive_comp/wutong/source/sim_data/sim_train_data/oppo_train')\n",
    "# ds.save_to_disk('/cognitive_comp/wutong/source/sim_data/sim_test_data/oppo')\n",
    "ds.save_to_disk('/cognitive_comp/wutong/source/sim_data/similarity_data/oppo_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets, json\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_oppo = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/sim_train_data/oppo_train_ds')\n",
    "dev_oppo = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/sim_test_data/oppo')\n",
    "test_oppo = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/similarity_data/oppo_test')\n",
    "oppo = datasets.concatenate_datasets([train_oppo, dev_oppo, test_oppo])\n",
    "oppo = oppo.shuffle(seed=42)\n",
    "\n",
    "with open('/cognitive_comp/wutong/source/sim_data/similarity_data/oppo_train_ds.json', 'w') as wp:\n",
    "    for idx in tqdm(range(oppo.num_rows)):\n",
    "        wp.write(json.dumps({'sentence': oppo[idx]['text1']}, ensure_ascii=False) + '\\n')\n",
    "        wp.write(json.dumps({'sentence': oppo[idx]['text2']}, ensure_ascii=False) + '\\n')\n",
    "        wp.flush()\n",
    "wp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = (datasets.load_dataset('json', data_files='/cognitive_comp/wutong/source/sim_data/similarity_data/oppo_train_ds.json',\n",
    "                            cache_dir='/cognitive_comp/wutong/source/data_base/huggingface-cache')['train'])\n",
    "ds.save_to_disk('/cognitive_comp/wutong/source/sim_data/predict_sentences/oppo_sentence')  # 454346"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 处理PAWS数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "paws_train = pd.read_csv('/cognitive_comp/wutong/source/sim_data/raw_data/paws_test.tsv', sep='\\t', header=None)\n",
    "with open('/cognitive_comp/wutong/source/sim_data/raw_data/paws_test.json', 'w') as wp:\n",
    "    for idx in tqdm(range(len(paws_train))):\n",
    "        wp.write(json.dumps({'text1': str(paws_train[0][idx]),\n",
    "                             'text2': str(paws_train[1][idx]),\n",
    "                            #  'score': int(paws_train[2][idx]),\n",
    "                             }, ensure_ascii=False) + '\\n')\n",
    "wp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "path = '/cognitive_comp/wutong/source/sim_data/raw_data/paws_test.json'\n",
    "feats = datasets.Features({\"text1\": datasets.Value('string'), \n",
    "                           \"text2\": datasets.Value('string'),\n",
    "                        #    \"score\": datasets.Value('int8')\n",
    "                           })\n",
    "ds = (datasets.load_dataset('json', data_files=path, \n",
    "                            cache_dir='/cognitive_comp/wutong/source/data_base/huggingface-cache',\n",
    "                            features=feats)['train'])\n",
    "# ds.save_to_disk('/cognitive_comp/wutong/source/sim_data/sim_train_data/paws_train')\n",
    "# ds.save_to_disk('/cognitive_comp/wutong/source/sim_data/sim_test_data/paws')\n",
    "ds.save_to_disk('/cognitive_comp/wutong/source/sim_data/similarity_data/paws_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets, json\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_paws = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/sim_train_data/paws_train')\n",
    "dev_paws = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/sim_test_data/paws')\n",
    "test_paws = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/similarity_data/paws_test')\n",
    "paws = datasets.concatenate_datasets([train_paws, dev_paws, test_paws])\n",
    "paws = paws.shuffle(seed=42)\n",
    "\n",
    "with open('/cognitive_comp/wutong/source/sim_data/similarity_data/paws_train_ds.json', 'w') as wp:\n",
    "    for idx in tqdm(range(paws.num_rows)):\n",
    "        wp.write(json.dumps({'sentence': paws[idx]['text1']}, ensure_ascii=False) + '\\n')\n",
    "        wp.write(json.dumps({'sentence': paws[idx]['text2']}, ensure_ascii=False) + '\\n')\n",
    "        wp.flush()\n",
    "wp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = (datasets.load_dataset('json', data_files='/cognitive_comp/wutong/source/sim_data/similarity_data/paws_train_ds.json',\n",
    "                            cache_dir='/cognitive_comp/wutong/source/data_base/huggingface-cache')['train'])\n",
    "ds.save_to_disk('/cognitive_comp/wutong/source/sim_data/predict_sentences/paws_sentence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 合并数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets, glob\n",
    "\n",
    "\n",
    "cache_dict_paths = glob.glob('/cognitive_comp/wutong/source/sim_data/similarity_data/sim_cache_data/*')\n",
    "ds = []\n",
    "for path in cache_dict_paths:\n",
    "    ds.append(datasets.load_from_disk(path))\n",
    "\n",
    "afqmc_train = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/sim_train_data/afqmc_train_ds')\n",
    "afqmc_dev = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/sim_test_data/afqmc')\n",
    "afqmc = datasets.concatenate_datasets([afqmc_train, afqmc_dev])\n",
    "ds.append(afqmc)\n",
    "\n",
    "chip_train = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/sim_train_data/chip_train_ds')\n",
    "chip_dev = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/sim_test_data/chip')\n",
    "chip = datasets.concatenate_datasets([chip_train, chip_dev])\n",
    "ds.append(chip)\n",
    "\n",
    "oppo_train = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/sim_train_data/oppo_train_ds')\n",
    "oppo_dev = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/sim_test_data/oppo')\n",
    "oppo = datasets.concatenate_datasets([oppo_train, oppo_dev])\n",
    "ds.append(oppo)\n",
    "\n",
    "print(len(ds))\n",
    "label_ds = datasets.concatenate_datasets(ds)\n",
    "label_ds.save_to_disk('/cognitive_comp/wutong/source/sim_data/similarity_data/labeled4paws')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/similarity_data/labeled_train_chip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "labeled_ds = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/similarity_data/labeled4chip')\n",
    "split_data = labeled_ds.train_test_split(test_size=0.02, seed=42)\n",
    "split_data['train'].save_to_disk('/cognitive_comp/wutong/source/sim_data/similarity_data/labeled_train_chip')\n",
    "split_data['test'].save_to_disk('/cognitive_comp/wutong/source/sim_data/similarity_data/labeled_test_chip')\n",
    "print(labeled_ds, split_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 其他Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "\n",
    "# data\n",
    "cands = ['我们都曾经年轻过']\n",
    "refs = ['我们都年少']\n",
    "\n",
    "P, R, F1 = score(cands, refs, lang=\"zh\", verbose=True)\n",
    "\n",
    "print(f\"System level F1 score = {F1.mean():.3f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# url=\"http://192.168.190.2:6631/davae\"\n",
    "url=\"http://192.168.52.173:23628/davae\"\n",
    "sents = ['当回首往事的时候，他不会因为虚度年华而悔恨，也不会因为碌碌无为而羞耻', '我心里那高兴劲啊,好像有一股甜滋滋清凉凉的风,掠过我的心头!']#,\n",
    "# json={ 'sent_inputs':sents, 'std_scale':0.5, 'delta_z':0.1, 'sent_goodcases':None, 'sent_badcases':None, 'augm_num':1, 'batch_size': 64, 'temperature': 1.0, 'top_k': 0,'top_p': 0.9,'max_out_length':128}\n",
    "result = requests.post(url,                                                                                               \n",
    "            json={\n",
    "                'sent_inputs': sents,\n",
    "                'top_p': 0.95,\n",
    "                # 'sent_badcases': sents,\n",
    "                'std_scale': 1.5,\n",
    "                'augm_num':2\n",
    "            }\n",
    "        ).json()\n",
    "\n",
    "print(result['generated_sentence'],  result['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url=\"http://192.168.52.151:42345/simgen\"\n",
    "sents = ['第九届新马领导人非正式峰会周二将在布城首相署举行。'] * 10\n",
    "result = requests.post(url,                                                                                               \n",
    "            json={\n",
    "                'sent_inputs': sents,\n",
    "                'top_p': 0.9,\n",
    "                'repetition_penalty': 1.0,\n",
    "                'max_out_length': 128\n",
    "            }\n",
    "        ).json()\n",
    "\n",
    "print(result['origin_sentence'])\n",
    "print(result['generated_sentence'])\n",
    "print(result['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2579772665e14a92e42a466d49ec9ff85683bc5df8d0c675aa9afdde4fd8e604"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
