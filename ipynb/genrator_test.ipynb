{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试Generator ZH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def gen_pred_collate(batch_data, gen_tokenizer):\n",
    "    input_ids, length_list = [], []\n",
    "    for item in batch_data:\n",
    "        cur_input_ids = gen_tokenizer(\n",
    "            '<bos>“' + item['text1'] + '”的相似句是“', return_tensors='pt'\n",
    "        ).input_ids.squeeze()[:-1]  # 不能加<eos>\n",
    "\n",
    "        # 每个样本复制 N 份\n",
    "        length = [cur_input_ids.size(0)] * 1\n",
    "        cur_input_ids = [cur_input_ids] * 1\n",
    "\n",
    "        length_list.extend(length)\n",
    "        input_ids.extend(cur_input_ids)\n",
    "\n",
    "    input_ids = pad_sequence(\n",
    "        [x for x in input_ids], batch_first=True, \n",
    "        padding_value=gen_tokenizer.pad_token_id)\n",
    "    length_tensor = torch.tensor(length_list)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'length_tensor': length_tensor,\n",
    "    }\n",
    "    \n",
    "# hyper parameters\n",
    "all_cycle = 8\n",
    "data_name = 'afqmc'\n",
    "ckpt_name = 'afqmc0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys, glob, datasets\n",
    "sys.path.append('/cognitive_comp/wutong/similarity_generation/')\n",
    "from data_utlis.sim_gen_dataset import SimGanDataset\n",
    "\n",
    "\n",
    "data = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/sim_test_data/' + data_name)\n",
    "\n",
    "gen_tokenizer = T5Tokenizer.from_pretrained(\n",
    "    '/cognitive_comp/wutong/source/model_base/chinese_sentencepiece/cog-pretrain.model',\n",
    "    eos_token='<|endoftext|>',\n",
    "    pad_token='<|endoftext|>',\n",
    "    extra_ids=0)\n",
    "gen_tokenizer.add_special_tokens({'bos_token': '<bos>'})\n",
    "\n",
    "predict_dataset = SimGanDataset(data)\n",
    "def collate_fn(batch_data):\n",
    "    return gen_pred_collate(batch_data, gen_tokenizer)\n",
    "dataloader = DataLoader(\n",
    "    dataset=predict_dataset,\n",
    "    batch_size=100,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, evaluate, torch\n",
    "sys.path.append('/cognitive_comp/wutong/similarity_generation/')\n",
    "from data_utlis.sample_sequence import sample_sequence_batch\n",
    "from model_utils.sim_gen_model import Generator\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "class Config:\n",
    "    cycle = 0\n",
    "    chinese = 1\n",
    "    txl_config_path = '/cognitive_comp/wutong/similarity_generation/model_utils/txl_5B_config.json'\n",
    "    txl_model_path = '/cognitive_comp/wutong/source/model_base/model_zh/txl_zh_5.0B.pt'\n",
    "    ckpt_model_path = '/cognitive_comp/wutong/similarity_generation/all_checkpoints/' + ckpt_name\n",
    "\n",
    "config = Config()\n",
    "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "raw_texts, sim_texts, mean_perplexity = [], [], []\n",
    "for idx in [0, 8]:\n",
    "    config.cycle = idx\n",
    "    generator = Generator(config)\n",
    "    generator.half().eval().cuda()\n",
    "    sim_sent_list = []\n",
    "    for batch in dataloader:\n",
    "        torch.cuda.empty_cache()\n",
    "        output_dict = sample_sequence_batch(\n",
    "            model=generator.gen, context_tokens_tensor=batch['input_ids'].cuda(),\n",
    "            context_length_tensor=batch['length_tensor'], repetition_penalty=1.0, max_out_seq=200,\n",
    "            end_token_id=50000, temperature=1.0, top_k=1, top_p=0.0,\n",
    "        )\n",
    "\n",
    "        sim_sent_list.extend(\n",
    "            gen_tokenizer.batch_decode(output_dict['ids_list'], skip_special_tokens=True))\n",
    "\n",
    "    raw_text, sim_text = [], []\n",
    "    for idx, item in enumerate(sim_sent_list):\n",
    "\n",
    "        item = item.replace(' ', '').split('”的相似句是“')\n",
    "        if item[0][1:] and item[1][:-1]:\n",
    "            raw_text.append(item[0][1:])\n",
    "            sim_text.append(item[1][:-1])\n",
    "    \n",
    "    raw_texts.append(raw_text)\n",
    "    sim_texts.append(sim_text)\n",
    "\n",
    "    mean_perplexity.append(perplexity.compute(input_texts=sim_text, model_id='gpt2')['mean_perplexity'])\n",
    "    print(mean_perplexity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")\n",
    "res_list = []\n",
    "for raw_text, sim_text in zip(raw_texts, sim_texts):\n",
    "    predictions = sim_text\n",
    "    references = raw_text\n",
    "    results = bertscore.compute(predictions=predictions, references=references, lang=\"zh\")['f1']\n",
    "    res_list.append(np.mean(results))\n",
    "print(res_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试Generator EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def gen_pred_collate(batch_data, gen_tokenizer):\n",
    "    input_ids, length_list = [], []\n",
    "    for item in batch_data:\n",
    "        cur_input_ids = gen_tokenizer(\n",
    "            '\"' + item['text1'] + '\" is similar to \"', return_tensors='pt'\n",
    "        ).input_ids.squeeze()[1:]  # 去掉<bos>\n",
    "\n",
    "        # 每个样本复制 N 份\n",
    "        length = [cur_input_ids.size(0)] * 1\n",
    "        cur_input_ids = [cur_input_ids] * 1\n",
    "\n",
    "        length_list.extend(length)\n",
    "        input_ids.extend(cur_input_ids)\n",
    "\n",
    "    if config.chinese:\n",
    "        input_ids = pad_sequence(\n",
    "            [x for x in input_ids], batch_first=True, \n",
    "            padding_value=gen_tokenizer.pad_token_id)\n",
    "    else:\n",
    "        input_ids = pad_sequence(\n",
    "            [x for x in input_ids], batch_first=True, \n",
    "            padding_value=gen_tokenizer.pad_token_id)\n",
    "    length_tensor = torch.tensor(length_list)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'length_tensor': length_tensor,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys, glob, datasets\n",
    "sys.path.append('/cognitive_comp/wutong/similarity_generation/')\n",
    "from data_utlis.sim_gen_dataset import SimGanDataset\n",
    "\n",
    "\n",
    "data = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/sim_test_data/mrpc')\n",
    "\n",
    "gen_tokenizer = GPT2Tokenizer.from_pretrained('/cognitive_comp/wutong/source/model_base/model_en/opt-2.7b')\n",
    "\n",
    "predict_dataset = SimGanDataset(data)\n",
    "def collate_fn(batch_data):\n",
    "    return gen_pred_collate(batch_data, gen_tokenizer)\n",
    "dataloader = DataLoader(\n",
    "    dataset=predict_dataset,\n",
    "    batch_size=300,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, evaluate, torch\n",
    "from tqdm import tqdm\n",
    "sys.path.append('/cognitive_comp/wutong/similarity_generation/')\n",
    "from model_utils.sim_gen_model import Generator_EN\n",
    "from data_utlis.sample_sequence import sample_sequence_batch_en\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "class Config:\n",
    "    cycle = 0\n",
    "    chinese = 0\n",
    "    opt_name = 'opt-2.7b'\n",
    "    opt_model_path = '/cognitive_comp/wutong/source/model_base/model_en/'\n",
    "    ckpt_model_path = '/cognitive_comp/wutong/similarity_generation/all_checkpoints/mrpc0'\n",
    "\n",
    "config = Config()\n",
    "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "raw_texts, sim_texts, mean_perplexity, sim_sent_list = [], [], [], []\n",
    "for idx in [0, 8]:\n",
    "    config.cycle = idx\n",
    "    generator = Generator_EN(config)\n",
    "    generator.half().eval().cuda()\n",
    "    for batch in dataloader:\n",
    "        torch.cuda.empty_cache()\n",
    "        output_dict = sample_sequence_batch_en(\n",
    "            model=generator.gen, context_tokens_tensor=batch['input_ids'].cuda(),\n",
    "            context_length_tensor=batch['length_tensor'], repetition_penalty=1.0, max_out_seq=100,\n",
    "            end_token_id=gen_tokenizer.eos_token_id, temperature=1.0, top_k=1, top_p=0.0,\n",
    "        )\n",
    "\n",
    "    sim_sent_list.extend(\n",
    "        gen_tokenizer.batch_decode(output_dict['ids_list'], skip_special_tokens=True))\n",
    "\n",
    "    raw_text, sim_text = [], []\n",
    "    for item in tqdm(sim_sent_list):\n",
    "        item = item.replace('\\n', '').split('\\\" is similar to \\\"')\n",
    "        if len(item) >= 2:\n",
    "            raw_text.append(item[0][1:])\n",
    "            if '\"' in item[0][1:]:\n",
    "                sim_text.append(item[1][:-1])\n",
    "            else:\n",
    "                sim_text.append(item[1].split('\"')[0])\n",
    "\n",
    "    raw_texts.append(raw_text)\n",
    "    sim_texts.append(sim_text)\n",
    "    \n",
    "    mean_perplexity.append(perplexity.compute(input_texts=sim_text, model_id='gpt2')['mean_perplexity'])\n",
    "    print(mean_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets, evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "for i in [0, 8]:\n",
    "    sim_text = []\n",
    "    data = datasets.load_from_disk('/cognitive_comp/wutong/similarity_generation/ipynb/afqmc/data_cycle_' + str(i))\n",
    "    for j in tqdm(range(data.num_rows)):\n",
    "        if data[j]['text2']:\n",
    "            sim_text.append(data[j]['text2'])\n",
    "    print(len(sim_text))\n",
    "    print(perplexity.compute(input_texts=sim_text, model_id='gpt2')['mean_perplexity'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "res_list = []\n",
    "bertscore = evaluate.load(\"bertscore\", module_type=\"metric\")\n",
    "for i in [0, 8]:\n",
    "    references, predictions = [], []\n",
    "    data = datasets.load_from_disk('/cognitive_comp/wutong/similarity_generation/ipynb/afqmc/data_cycle_' + str(i))\n",
    "    for j in tqdm(range(data.num_rows)):\n",
    "        if data[j]['text1'] and data[j]['text2']:\n",
    "            references.append(data[j]['text1'])\n",
    "            predictions.append(data[j]['text2'])\n",
    "    print(len(references))\n",
    "    results = bertscore.compute(predictions=predictions, references=references, lang=\"zh\")['f1']\n",
    "    res_list.append(np.mean(results))\n",
    "print(res_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict(tokens, ngram, gdict=None):\n",
    "    \"\"\"\n",
    "    get_dict\n",
    "    统计n-gram频率并用dict存储\n",
    "    \"\"\"\n",
    "    token_dict = {}\n",
    "    if gdict is not None:\n",
    "        token_dict = gdict\n",
    "    tlen = len(tokens)\n",
    "    for i in range(0, tlen - ngram + 1):\n",
    "        ngram_token = \"\".join(tokens[i:(i + ngram)])\n",
    "        if token_dict.get(ngram_token) is not None: \n",
    "            token_dict[ngram_token] += 1\n",
    "        else:\n",
    "            token_dict[ngram_token] = 1\n",
    "    return token_dict\n",
    "\n",
    "\n",
    "def calc_distinct_ngram(pair_list, ngram):\n",
    "    \"\"\"\n",
    "    calc_distinct_ngram\n",
    "    \"\"\"\n",
    "    ngram_total = 0.0\n",
    "    ngram_distinct_count = 0.0\n",
    "    pred_dict = {}\n",
    "    for predict_tokens, _ in pair_list:\n",
    "        get_dict(predict_tokens, ngram, pred_dict)\n",
    "    for key, freq in pred_dict.items():\n",
    "        ngram_total += freq\n",
    "        ngram_distinct_count += 1 \n",
    "        #if freq == 1:\n",
    "        #    ngram_distinct_count += freq\n",
    "    return ngram_distinct_count / ngram_total\n",
    "\n",
    "\n",
    "def calc_distinct(pair_list):\n",
    "    \"\"\"\n",
    "    calc_distinct\n",
    "    \"\"\"\n",
    "    distinct1 = calc_distinct_ngram(pair_list, 3)\n",
    "    distinct2 = calc_distinct_ngram(pair_list, 4)\n",
    "    return [distinct1, distinct2]\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "def count(pred_tokens, gold_tokens, ngram, result):\n",
    "    \"\"\"\n",
    "    计算BLEU中pn\n",
    "    \"\"\"\n",
    "    cover_count, total_count = result\n",
    "    pred_dict = get_dict(pred_tokens, ngram)\n",
    "    gold_dict = get_dict(gold_tokens, ngram)\n",
    "    cur_cover_count = 0\n",
    "    cur_total_count = 0\n",
    "    for token, freq in pred_dict.items():\n",
    "        if gold_dict.get(token) is not None:\n",
    "            gold_freq = gold_dict[token]\n",
    "            cur_cover_count += min(freq, gold_freq)\n",
    "        cur_total_count += freq\n",
    "    result[0] += cur_cover_count\n",
    "    result[1] += cur_total_count\n",
    "\n",
    "\n",
    "def calc_bp(pair_list):\n",
    "    \"\"\"\n",
    "    calc_bp\n",
    "    \"\"\"\n",
    "    c_count = 0.0\n",
    "    r_count = 0.0\n",
    "    for pair in pair_list:\n",
    "        pred_tokens, gold_tokens = pair\n",
    "        c_count += len(pred_tokens)\n",
    "        r_count += len(gold_tokens)\n",
    "    bp = 1\n",
    "    if c_count < r_count:\n",
    "        bp = math.exp(1 - r_count / c_count)\n",
    "    return bp \n",
    "\n",
    "\n",
    "def calc_cover_rate(pair_list, ngram):\n",
    "    \"\"\"\n",
    "    calc_cover_rate\n",
    "    \"\"\"\n",
    "    result = [0.0, 0.0] # [cover_count, total_count]\n",
    "    for pair in pair_list:\n",
    "        pred_tokens, gold_tokens = pair\n",
    "        count(pred_tokens, gold_tokens, ngram, result)\n",
    "    cover_rate = result[0] / result[1]\n",
    "    return cover_rate \n",
    "\n",
    "\n",
    "def calc_bleu(pair_list):\n",
    "    \"\"\"\n",
    "    calc_bleu\n",
    "    \"\"\"\n",
    "    bp = calc_bp(pair_list)\n",
    "    cover_rate1 = calc_cover_rate(pair_list, 1)\n",
    "    cover_rate2 = calc_cover_rate(pair_list, 2)\n",
    "    cover_rate3 = calc_cover_rate(pair_list, 3)\n",
    "    bleu1 = 0\n",
    "    bleu2 = 0\n",
    "    bleu3 = 0\n",
    "    if cover_rate1 > 0:\n",
    "        bleu1 = bp * math.exp(math.log(cover_rate1))\n",
    "    if cover_rate2 > 0:\n",
    "        bleu2 = bp * math.exp((math.log(cover_rate1) + math.log(cover_rate2)) / 2)\n",
    "    if cover_rate3 > 0:\n",
    "        bleu3 = bp * math.exp((math.log(cover_rate1) + math.log(cover_rate2) + math.log(cover_rate3)) / 3)\n",
    "    return [bleu1, bleu2, bleu3]\n",
    "\n",
    "\n",
    "import jieba\n",
    "\n",
    "dist1, dist2 = [], []\n",
    "bleu1_list, bleu2_list, bleu3_list = [], [], []\n",
    "for raw_text, sim_text in zip(raw_texts, sim_texts):\n",
    "    sents = []\n",
    "    for idx in range(10):\n",
    "        seg_list = jieba.cut(raw_text[idx])\n",
    "        gold_tokens = \" \".join(seg_list).strip().split(\" \")\n",
    "        \n",
    "        seg_list = jieba.cut(sim_text[idx])\n",
    "        pred_tokens = \" \".join(seg_list).strip().split(\" \")\n",
    "        sents.append([pred_tokens, gold_tokens])\n",
    "    \n",
    "    bleu1, bleu2, bleu3 = calc_bleu(sents)\n",
    "    distinct1, distinct2 = calc_distinct(sents)\n",
    "    \n",
    "    dist1.append(distinct1)\n",
    "    dist2.append(distinct2)\n",
    "    bleu1_list.append(bleu1)\n",
    "    bleu2_list.append(bleu2)\n",
    "    bleu3_list.append(bleu3)\n",
    "\n",
    "print(dist1)\n",
    "print(dist2)\n",
    "print(bleu1_list)\n",
    "print(bleu2_list)\n",
    "print(bleu3_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample ZH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "\n",
    "gen_tokenizer = T5Tokenizer.from_pretrained(\n",
    "    '/cognitive_comp/wutong/source/model_base/chinese_sentencepiece/cog-pretrain.model',\n",
    "    eos_token='<|endoftext|>',\n",
    "    pad_token='<|endoftext|>',\n",
    "    extra_ids=0)\n",
    "gen_tokenizer.add_special_tokens({'bos_token': '<bos>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import json\n",
    "import sys\n",
    "sys.path.append('/cognitive_comp/wutong/similarity_generation/')\n",
    "from model_utils.gpt2_modeling import GPT2Model\n",
    "\n",
    "\n",
    "with open('/cognitive_comp/wutong/similarity_generation/model_utils/txl_5B_config.json', 'r') as f:\n",
    "    txl_config = json.load(f)\n",
    "gen = GPT2Model(\n",
    "    num_layers=txl_config['num_layers'],\n",
    "    vocab_size=txl_config['vocab_size'],\n",
    "    hidden_size=txl_config['hidden_size'],\n",
    "    num_attention_heads=txl_config['num_attention_heads'],\n",
    "    embedding_dropout_prob=txl_config['embedding_dropout_prob'],\n",
    "    attention_dropout_prob=txl_config['attention_dropout_prob'],\n",
    "    output_dropout_prob=txl_config['output_dropout_prob'],\n",
    "    max_sequence_length=txl_config['max_sequence_length'],\n",
    "    max_memory_length=txl_config['max_memory_length'],\n",
    "    checkpoint_activations=txl_config['checkpoint_activations'],\n",
    "    checkpoint_num_layers=txl_config['checkpoint_num_layers'],\n",
    "    parallel_output=txl_config['parallel_output'],\n",
    "    relative_encoding=txl_config['relative_encoding']\n",
    ")\n",
    "\n",
    "\n",
    "def load_model(cycle):\n",
    "    ckpt_path = '/cognitive_comp/wutong/similarity_generation/all_checkpoints/new_exp6'\n",
    "    pt_path = ckpt_path +\\\n",
    "        f'/generator_cycle_{cycle}.ckpt/checkpoint/mp_rank_00_model_states.pt'\n",
    "    new_dict = {}\n",
    "    state_dict = torch.load(pt_path, map_location='cpu')['module']\n",
    "    for k, v in state_dict.items():\n",
    "        if any([i in k for i in ['module.generator.gen.']]):\n",
    "            new_dict[k[len('module.generator.gen.'):]] = v\n",
    "        else:\n",
    "            continue\n",
    "    if new_dict == {}:\n",
    "        new_dict = state_dict\n",
    "    gen.load_state_dict(new_dict)\n",
    "    print('The Generator Transformer-XL Load Successfully !\\n')\n",
    "    \n",
    "    return gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import sys\n",
    "sys.path.append('/cognitive_comp/wutong/similarity_generation/')\n",
    "from data_utlis.sample_sequence import sample_sequence_batch\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# input_ids, length_list = [], []\n",
    "# data = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/sim_test_data/qqp')\n",
    "# for idx in range(80, 110):\n",
    "#     cur_input_ids = gen_tokenizer(\n",
    "#         '<bos>“' + data[idx]['text1'] + '”的相似句是“', return_tensors='pt'\n",
    "#     ).input_ids.squeeze()[:-1]  # 不能加<eos>\n",
    "\n",
    "#     length = [cur_input_ids.size(0)]\n",
    "#     cur_input_ids = [cur_input_ids]\n",
    "\n",
    "#     length_list.extend(length)\n",
    "#     input_ids.extend(cur_input_ids)\n",
    "\n",
    "# input_ids = pad_sequence(\n",
    "#     [x for x in input_ids], batch_first=True, padding_value=50000)\n",
    "# length_tensor = torch.tensor(length_list)\n",
    "\n",
    "input_ids = gen_tokenizer(\n",
    "        '“透明质酸具有优异的生物相容性和主动肿瘤靶向性, 可被透明质酸酶降解。”的相似句是“', return_tensors='pt').input_ids[:, :-1]  # 不能加<eos>\n",
    "length_tensor = torch.tensor([input_ids.size(1)])\n",
    "print('input_ids', input_ids.size())\n",
    "\n",
    "gen = load_model(1).half().eval().cuda()\n",
    "output_dict = sample_sequence_batch(\n",
    "    model=gen, context_tokens_tensor=input_ids.cuda(),\n",
    "    context_length_tensor=length_tensor, repetition_penalty=1.0, max_out_seq=200,\n",
    "    end_token_id=50000, temperature=1.0, top_k=0, top_p=0.95,\n",
    ")\n",
    "print(gen_tokenizer.batch_decode(output_dict['ids_list'], skip_special_tokens=True))\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "load_model(1).half().cuda().eval()\n",
    "output_dict = sample_sequence_batch(\n",
    "    model=gen, context_tokens_tensor=input_ids.cuda(),\n",
    "    context_length_tensor=length_tensor, repetition_penalty=1.0, max_out_seq=200,\n",
    "    end_token_id=50000, temperature=1.0, top_k=0, top_p=0.95,\n",
    ")\n",
    "print(gen_tokenizer.batch_decode(output_dict['ids_list'], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "from transformers import OPTForCausalLM, GPT2Tokenizer\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'\n",
    "\n",
    "model = OPTForCausalLM.from_pretrained('/cognitive_comp/wutong/source/model_base/model_en/opt-2.7b')\n",
    "state_dict = torch.load('/cognitive_comp/wutong/source/model_base/model_en/opt-2.7b.pt',\n",
    "                        map_location='cpu')['module']\n",
    "new_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    if any([i in k for i in ['module.generator.gen.']]):\n",
    "        new_dict[k[len('module.generator.gen.'):]] = v\n",
    "    else:\n",
    "        continue\n",
    "if new_dict == {}:\n",
    "    new_dict = state_dict\n",
    "model.load_state_dict(new_dict)\n",
    "model = model.to('cuda')\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('/cognitive_comp/wutong/source/model_base/model_en/opt-2.7b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "train_paws = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/sim_train_data/paws_train_ds')\n",
    "train_paws = train_paws.select(range(5000))\n",
    "train_paws.save_to_disk('/cognitive_comp/wutong/source/sim_data/similarity_data_en/paws_tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt, text = [], []\n",
    "for i in range(10):\n",
    "    if train_paws[i]['score'] == 1:\n",
    "        prompt.append('\"' + train_paws[i]['text1'] + '\" is similar to \"')\n",
    "        text.append(train_paws[i]['text2'])\n",
    "inputs = tokenizer.batch_encode_plus(prompt, padding=True, return_tensors='pt')\n",
    "generate_ids = model.generate(inputs.input_ids.cuda(), top_p=0.8, max_length=200, repetition_penalty=1.0)\n",
    "res = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(res)):\n",
    "    print(res[i])\n",
    "    print(text[i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2579772665e14a92e42a466d49ec9ff85683bc5df8d0c675aa9afdde4fd8e604"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
