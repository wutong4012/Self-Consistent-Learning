{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def padding_dis_mask(mam: list, max_length):\n",
    "    for idx in range(len(mam)):\n",
    "        padding_length = max_length - mam[idx].size(0)\n",
    "        up_padding = (0, 0, 0, padding_length)  # 三角阵正下方补0\n",
    "        mam[idx] = F.pad(mam[idx], up_padding, value=0)\n",
    "        right_padding = (0, padding_length, 0, 0)  # 三角阵正右方补0\n",
    "        mam[idx] = F.pad(mam[idx], right_padding, value=0)\n",
    "\n",
    "    mam = torch.stack(mam)  # stack: list[tensor] -> tensor\n",
    "    return mam  # [batch_size, max_len, max_len+M]\n",
    "\n",
    "\n",
    "def dis_pred_collate(batch_data, tokenizer):\n",
    "    max_length = 0\n",
    "    input_ids, token_type_ids, attention_mask, position_ids = [], [], [], []\n",
    "    clslabels_mask, labels, label_idx = [], [], []\n",
    "    for item in batch_data:\n",
    "        max_length = max(max_length, item['attention_mask'].size(0))\n",
    "        input_ids.append(item['input_ids'])\n",
    "        token_type_ids.append(item['token_type_ids'])\n",
    "        attention_mask.append(item['attention_mask'])\n",
    "        position_ids.append(item['position_ids'])\n",
    "        clslabels_mask.append(item['clslabels_mask'])\n",
    "        labels.append(item['label'])\n",
    "        label_idx.append(item['label_idx'])\n",
    "    \n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    token_type_ids = pad_sequence(token_type_ids, batch_first=True, padding_value=0)\n",
    "    attention_mask = padding_dis_mask(attention_mask, max_length)\n",
    "    position_ids = pad_sequence(position_ids, batch_first=True, padding_value=0)\n",
    "    clslabels_mask = pad_sequence(clslabels_mask, batch_first=True, padding_value=-10000)\n",
    "        \n",
    "    return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"position_ids\": position_ids,\n",
    "            \"clslabels_mask\": clslabels_mask,\n",
    "            'label_idx': torch.stack(label_idx),\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "def preprocess(sample):\n",
    "    sample['text1'] = sample['sentence1']\n",
    "    sample['text2'] = ''\n",
    "    sample['question'] = '怎么理解这段话？'\n",
    "    sample['choice'] = [\"不能理解为：\"+sample['sentence2'],\n",
    "                        \"可以理解为：\"+sample['sentence2']]\n",
    "    return sample\n",
    "\n",
    "\n",
    "def get_att_mask(attention_mask, label_idx, question_len):\n",
    "    max_length = len(attention_mask)\n",
    "    attention_mask = np.array(attention_mask)\n",
    "    attention_mask = np.tile(attention_mask[None, :], (max_length, 1))\n",
    "\n",
    "    zeros = np.zeros(\n",
    "        shape=(label_idx[-1]-question_len, label_idx[-1]-question_len))\n",
    "    \n",
    "    attention_mask[question_len:label_idx[-1],\n",
    "                   question_len:label_idx[-1]] = zeros\n",
    "\n",
    "    for i in range(len(label_idx)-1):\n",
    "        label_token_length = label_idx[i+1]-label_idx[i]\n",
    "        if label_token_length <= 0:\n",
    "            print('label_idx', label_idx)\n",
    "            print('question_len', question_len)\n",
    "            continue\n",
    "        ones = np.ones(shape=(label_token_length, label_token_length))\n",
    "        attention_mask[label_idx[i]:label_idx[i+1],\n",
    "                       label_idx[i]:label_idx[i+1]] = ones\n",
    "\n",
    "    return attention_mask\n",
    "\n",
    "\n",
    "def get_position_ids(label_idx, max_length, question_len):\n",
    "    question_position_ids = np.arange(question_len)\n",
    "    label_position_ids = np.arange(question_len, label_idx[-1])\n",
    "    for i in range(len(label_idx)-1):\n",
    "        label_position_ids[label_idx[i]-question_len:label_idx[i+1]-question_len] = np.arange(\n",
    "            question_len, question_len+label_idx[i+1]-label_idx[i])\n",
    "    max_len_label = max(label_position_ids)\n",
    "    text_position_ids = np.arange(\n",
    "        max_len_label+1, max_length+max_len_label+1-label_idx[-1])\n",
    "    position_ids = list(question_position_ids) + \\\n",
    "        list(label_position_ids)+list(text_position_ids)\n",
    "    if max_length <= 512:\n",
    "        return position_ids[:max_length]\n",
    "    else:\n",
    "        for i in range(512, max_length):\n",
    "            if position_ids[i] > 511:\n",
    "                position_ids[i] = 511\n",
    "        return position_ids[:max_length]\n",
    "\n",
    "\n",
    "def random_masking(token_ids, mask_rate, mask_start_idx, max_length, tokenizer=None):\n",
    "    rands = np.random.random(len(token_ids))\n",
    "    source, target, mask_pos = [], [], []\n",
    "\n",
    "    # 删除-CLS SEP id\n",
    "    mask_id = tokenizer.convert_tokens_to_ids(['[MASK]'])[0]\n",
    "    cls_id = tokenizer.convert_tokens_to_ids([\"[CLS]\"])[0]\n",
    "    sep_id = tokenizer.convert_tokens_to_ids([\"[SEP]\"])[0]\n",
    "\n",
    "    for i, (r, t) in enumerate(zip(rands, token_ids)):\n",
    "        if i < mask_start_idx:\n",
    "            source.append(t)\n",
    "            target.append(-100)\n",
    "            continue\n",
    "        if t == cls_id or t == sep_id:\n",
    "            source.append(t)\n",
    "            target.append(-100)\n",
    "            continue\n",
    "\n",
    "        if r < mask_rate * 0.8:\n",
    "            source.append(mask_id)\n",
    "            target.append(t)\n",
    "            mask_pos.append(i)\n",
    "        elif r < mask_rate * 0.9:\n",
    "            source.append(t)\n",
    "            target.append(t)\n",
    "            mask_pos.append(i)\n",
    "        elif r < mask_rate:\n",
    "            source.append(np.random.choice(\n",
    "                len(tokenizer.get_vocab().keys()) - 2) + 1)\n",
    "            target.append(t)\n",
    "            mask_pos.append(i)\n",
    "        else:\n",
    "            source.append(t)\n",
    "            target.append(-100)\n",
    "\n",
    "    while len(source) < max_length:\n",
    "        source.append(0)\n",
    "        target.append(-100)\n",
    "    return source[:max_length], target[:max_length], mask_pos\n",
    "\n",
    "\n",
    "class SimGanDataset(Dataset):\n",
    "    \"\"\"\n",
    "        labeled Data(datasets): text1(str), text2(str), label(int8) \n",
    "        Generated Data(datasets): text1(str), text2(str), label(int8) \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, tokenizer=None, predict=False, is_gen=False, test=False) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.test = test\n",
    "        self.is_gen = is_gen\n",
    "        self.predict = predict\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        if self.is_gen:\n",
    "            return item\n",
    "\n",
    "        one_data = self.predict_encoder(item, self.tokenizer)\n",
    "\n",
    "        return one_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.num_rows\n",
    "\n",
    "    def predict_encoder(self, item, t_idx):\n",
    "        item['label'] = int(item['label'])\n",
    "        tokenizer = self.tokenizer\n",
    "        yes_token_id = tokenizer.encode(\"是\")[1]\n",
    "        no_token_id = tokenizer.encode(\"非\")[1]\n",
    "\n",
    "        # item['question']='[CLS]'\n",
    "        if item['text2']!='':\n",
    "            texta =  '[MASK]' + '[MASK]'.join(item['choice'])+ '[SEP]' +item['question'] + '[SEP]' +item[ 'text1']\n",
    "            # texta =  item['question'] + '[SEP]' +'[MASK]' + '[MASK]'.join(item['choice'])+ '[SEP]'+item[ 'text1']+'[SEP]'+item['text2']\n",
    "            encode_dict = tokenizer.encode_plus(texta,\n",
    "                                                max_length=512,\n",
    "                                                padding=\"longest\",\n",
    "                                                truncation=True\n",
    "                                                )\n",
    "        else:\n",
    "            texta =  '[MASK]' + '[MASK]'.join(item['choice'])+ '[SEP]'+item['question'] + '[SEP]' +item[ 'text1']\n",
    "            encode_dict = tokenizer.encode_plus(texta,\n",
    "                                                max_length=512,\n",
    "                                                padding=\"longest\",\n",
    "                                                truncation=True\n",
    "                                                )\n",
    "        \n",
    "        encode_sent = encode_dict['input_ids']\n",
    "        token_type_ids=encode_dict['token_type_ids']\n",
    "        attention_mask=encode_dict['attention_mask']\n",
    "        \n",
    "        # question_len=len(self.tokenizer.encode(item['question']))\n",
    "        question_len=1\n",
    "        label_idx=[question_len]\n",
    "        for choice in item['choice']:\n",
    "            cur_mask_idx=label_idx[-1]+len(tokenizer.encode(choice,add_special_tokens=False))+1\n",
    "            label_idx.append(cur_mask_idx)\n",
    "\n",
    "        encoded_len = len(encode_dict[\"input_ids\"])\n",
    "        zero_len = len(encode_dict[\"input_ids\"]) - question_len - ((label_idx[-1]-label_idx[0]+1))\n",
    "        token_type_ids=[0]*question_len+[1]*(label_idx[-1]-label_idx[0]+1)+[0]*zero_len\n",
    "\n",
    "        attention_mask=get_att_mask(attention_mask,label_idx,question_len)\n",
    "\n",
    "        position_ids=get_position_ids(label_idx,encoded_len,question_len)\n",
    "        \n",
    "        clslabels_mask = np.zeros(shape=(len(encode_sent),))\n",
    "        clslabels_mask[label_idx[:-1]]=10000\n",
    "        clslabels_mask=clslabels_mask-10000\n",
    "\n",
    "        mlmlabels_mask=np.zeros(shape=(len(encode_sent),))\n",
    "        mlmlabels_mask[label_idx[0]]=1\n",
    "\n",
    "        # used_mask=False\n",
    "        source, target = encode_sent[:], encode_sent[:]\n",
    "        \n",
    "        source=np.array(source)\n",
    "        target=np.array(target)\n",
    "        source[label_idx[:-1]]=tokenizer.mask_token_id\n",
    "        target[label_idx[:-1]]=no_token_id\n",
    "        \n",
    "        target[label_idx[item['label']]]=yes_token_id\n",
    "        clslabels = label_idx[item['label']]\n",
    "        # target[label_idx[:-1]]=-100\n",
    "        # target[label_idx[item['label']]]=-100\n",
    "\n",
    "        end_token = [\"[SEP]\"]\n",
    "        end_id = tokenizer.convert_tokens_to_ids(end_token)[0]   \n",
    "        seq_actual_len = len(source) - list(source[::-1]).index(end_id)\n",
    "\n",
    "        one_data = {\n",
    "            \"id\": item[\"id\"],\n",
    "            \"input_ids\": torch.tensor(source).long(),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids).long(),\n",
    "            \"attention_mask\": torch.tensor(attention_mask).float(),\n",
    "            \"position_ids\":torch.tensor(position_ids).long(),\n",
    "            \"mlmlabels\": torch.tensor(target).long(),\n",
    "            \"clslabels\": torch.tensor(clslabels).long(),\n",
    "            \"clslabels_mask\": torch.tensor(clslabels_mask).float(),\n",
    "            \"mlmlabels_mask\": torch.tensor(mlmlabels_mask).float(),\n",
    "            \"label_idx\": torch.tensor(label_idx).long(),\n",
    "            'label': item['label']\n",
    "        }\n",
    "\n",
    "        return one_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, config, tokenizer) -> None:\n",
    "        super().__init__()\n",
    "        pt_path = '/cognitive_comp/wutong/source/model_base/pretrained_zh/macbert_large_mc'  ## 改成你自己的模型地址（macbert）\n",
    "        self.bert_encoder = AutoModelForMaskedLM.from_pretrained(pt_path) \n",
    "\n",
    "        self.yes_token = tokenizer.encode(\"是\")[1]\n",
    "        self.no_token = tokenizer.encode(\"非\")[1]\n",
    "\n",
    "        self.loss_func = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "        self.KL_criterion = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "        self.temperature = 1\n",
    "        self.do_annealing = None\n",
    "\n",
    "        if config.warm_up_model:\n",
    "            print('Use Warm Up Model...')\n",
    "            if config.cycle == 0 or config.cycle == -1:\n",
    "                pt_path = '/cognitive_comp/wutong/finetune_large.bin'  ## 改成你自己的模型地址（finetune）\n",
    "                state_dict = torch.load(pt_path, map_location='cpu')\n",
    "                new_dict = {}\n",
    "                for k, v in state_dict.items():\n",
    "                    if any([i in k for i in ['bert_encoder.']]):\n",
    "                        new_dict[k[len('bert_encoder.'):]] = v\n",
    "                    else:\n",
    "                        continue\n",
    "                self.bert_encoder.load_state_dict(new_dict)\n",
    "                print(f'The warm up model path is {pt_path}!')\n",
    "        \n",
    "        print(f'Cycle {config.cycle}: The Discriminator Load Successfully !\\n')\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, position_ids, clslabels_mask, bt_label_idx):\n",
    "      \n",
    "        bert_output = self.bert_encoder(input_ids=input_ids,\n",
    "                                        attention_mask=attention_mask,\n",
    "                                        token_type_ids=token_type_ids,\n",
    "                                        position_ids=position_ids,\n",
    "                                        labels=None,)\n",
    "        \n",
    "        _,seq_len=input_ids.shape\n",
    "        cls_logits = bert_output.logits[:, :,self.yes_token].view(-1, seq_len) + clslabels_mask\n",
    "        \n",
    "        label_idx = bt_label_idx[:,:-1]\n",
    "        \n",
    "        probs = torch.nn.functional.softmax(cls_logits, dim=-1)  \n",
    "        probs_ = torch.gather(probs, dim=1, index=label_idx)\n",
    "        pred_labels = torch.argmax(probs_, dim=-1)\n",
    "        \n",
    "        return probs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-04d94b320575d4be\n",
      "Reusing dataset json (/home/wutong/.cache/huggingface/datasets/json/default-04d94b320575d4be/0.0.0)\n",
      "Loading cached processed dataset at /home/wutong/.cache/huggingface/datasets/json/default-04d94b320575d4be/0.0.0/cache-cbd05c05334956c1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Warm Up Model...\n",
      "The warm up model path is /cognitive_comp/wutong/finetune_large.bin!\n",
      "Cycle 0: The Discriminator Load Successfully !\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0.8051668460710442\n",
      "0.7957110609480813\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'\n",
    "\n",
    "import datasets, torch\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "\n",
    "class Config:\n",
    "    cycle = 0\n",
    "    warm_up_model = True\n",
    "    \n",
    "config = Config()\n",
    "dis_tokenizer = AutoTokenizer.from_pretrained('/cognitive_comp/wutong/source/model_base/pretrained_zh/macbert_large_mc')  ## 改成你自己的模型地址（macbert）\n",
    "\n",
    "test_data = datasets.Dataset.from_json('/cognitive_comp/wutong/source/sim_data/raw_data/bustm/test_public.json')  ## 改成你自己的数据地址\n",
    "test_data = test_data.map(preprocess)\n",
    "test_dataset = SimGanDataset(data=test_data, tokenizer=dis_tokenizer, test=True)\n",
    "def collate_fn(batch_data):\n",
    "    return dis_pred_collate(batch_data, dis_tokenizer)\n",
    "dataloader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=512,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "pred_result = []\n",
    "f1_result, acc_result = [], []\n",
    "all_labels, all_preds = [], []\n",
    "\n",
    "config.cycle = 0\n",
    "discriminator = Discriminator(config, dis_tokenizer)\n",
    "discriminator.cuda().eval()\n",
    "with torch.no_grad():\n",
    "    pred_list = []\n",
    "    f1_score_list, acc_score_list = [], []\n",
    "    for batch in dataloader:\n",
    "        all_logits = []\n",
    "        torch.cuda.empty_cache()\n",
    "        prob = discriminator.forward(\n",
    "            input_ids=batch['input_ids'].cuda(),\n",
    "            attention_mask=batch['attention_mask'].cuda(),\n",
    "            token_type_ids=batch['token_type_ids'].cuda(),\n",
    "            position_ids=batch['position_ids'].cuda(),\n",
    "            clslabels_mask=batch['clslabels_mask'].cuda(),\n",
    "            bt_label_idx=batch['label_idx'].cuda()\n",
    "        )\n",
    "        \n",
    "        predictions = torch.argmax(prob, dim=1).tolist()\n",
    "        all_labels.extend(batch['labels'])\n",
    "        all_preds.extend(predictions)\n",
    "        \n",
    "    print(f1_score(all_labels, all_preds))\n",
    "    f1_result.append(f1_score(all_labels, all_preds))\n",
    "    print(accuracy_score(all_labels, all_preds))\n",
    "    acc_result.append(accuracy_score(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8002257336343115\n"
     ]
    }
   ],
   "source": [
    "your_preds = [1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1]\n",
    "your_trues = [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "print(accuracy_score(your_preds, your_trues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "your_preds == all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2579772665e14a92e42a466d49ec9ff85683bc5df8d0c675aa9afdde4fd8e604"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
