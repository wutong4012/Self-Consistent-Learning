{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "model = AutoModel.from_pretrained(\"facebook/opt-350m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.save_pretrained(\"/cognitive_comp/wutong/source/model_base/model_en/opt_350m.pt\")\n",
    "_ = tokenizer.save_pretrained(\"/cognitive_comp/wutong/source/model_base/model_en/opt_350m.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num = sum(p.numel() for p in model.parameters())\n",
    "print(total_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### QQP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "qqp_train = pd.read_csv('/cognitive_comp/wutong/source/sim_data/raw_data_en/QQP/qqp_train.tsv', sep='\\t')\n",
    "qqp_dev = pd.read_csv('/cognitive_comp/wutong/source/sim_data/raw_data_en/QQP/qqp_dev.tsv', sep='\\t')\n",
    "with open('/cognitive_comp/wutong/source/sim_data/raw_data_en/qqp_sim_data.json', 'w') as wp:\n",
    "    for idx in tqdm(range(len(qqp_train))):\n",
    "        if qqp_train['is_duplicate'].iloc[idx] == 1 and str(qqp_train['question1'].iloc[idx]) != \"\" and str(qqp_train['question1'].iloc[idx]) != \"\":\n",
    "            wp.write(json.dumps({'text1': str(qqp_train['question1'].iloc[idx]),\n",
    "                                'text2': str(qqp_train['question2'].iloc[idx]),\n",
    "                                # 'score': int(qqp_train['is_duplicate'].iloc[idx]),\n",
    "                                }, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    for idx in tqdm(range(len(qqp_dev))):\n",
    "        if qqp_dev['is_duplicate'].iloc[idx] == 1 and str(qqp_dev['question1'].iloc[idx]) != \"\" and str(qqp_dev['question1'].iloc[idx]) != \"\":\n",
    "            wp.write(json.dumps({'text1': str(qqp_dev['question1'].iloc[idx]),\n",
    "                                 'text2': str(qqp_dev['question2'].iloc[idx]),\n",
    "                                # 'score': int(qqp_dev['is_duplicate'].iloc[idx]),\n",
    "                                }, ensure_ascii=False) + '\\n')\n",
    "wp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "path = '/cognitive_comp/wutong/source/sim_data/raw_data_en/qqp_sim_data.json'\n",
    "feats = datasets.Features({\"text1\": datasets.Value('string'), \n",
    "                           \"text2\": datasets.Value('string'),\n",
    "                           })\n",
    "ds = (datasets.load_dataset('json', data_files=path, \n",
    "                            cache_dir='/cognitive_comp/wutong/source/data_base/huggingface-cache',\n",
    "                            features=feats)['train'])\n",
    "ds.save_to_disk('/cognitive_comp/wutong/source/sim_data/similarity_data_en/qqp_sim_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "qqp_data = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/similarity_data_en/qqp_sim_data')\n",
    "split_data = qqp_data.train_test_split(test_size=0.08, seed=42)\n",
    "split_data['train'].save_to_disk('/cognitive_comp/wutong/source/sim_data/similarity_data_en/qqp_train')\n",
    "split_data['test'].save_to_disk('/cognitive_comp/wutong/source/sim_data/similarity_data_en/qqp_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "qqp_train = pd.read_csv('/cognitive_comp/wutong/source/sim_data/raw_data_en/QQP/qqp_train.tsv', sep='\\t')\n",
    "qqp_dev = pd.read_csv('/cognitive_comp/wutong/source/sim_data/raw_data_en/QQP/qqp_dev.tsv', sep='\\t')\n",
    "with open('/cognitive_comp/wutong/source/sim_data/raw_data_en/qqp_sim_data.json', 'w') as wp:\n",
    "    for idx in tqdm(range(len(qqp_train))):\n",
    "        if str(qqp_train['question1'].iloc[idx]) != \"\" and str(qqp_train['question1'].iloc[idx]) != \"\":\n",
    "            wp.write(json.dumps({'text1': str(qqp_train['question1'].iloc[idx]),\n",
    "                                'text2': str(qqp_train['question2'].iloc[idx]),\n",
    "                                'score': int(qqp_train['is_duplicate'].iloc[idx]),\n",
    "                                }, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    for idx in tqdm(range(len(qqp_dev))):\n",
    "        if str(qqp_dev['question1'].iloc[idx]) != \"\" and str(qqp_dev['question1'].iloc[idx]) != \"\":\n",
    "            wp.write(json.dumps({'text1': str(qqp_dev['question1'].iloc[idx]),\n",
    "                                 'text2': str(qqp_dev['question2'].iloc[idx]),\n",
    "                                'score': int(qqp_dev['is_duplicate'].iloc[idx]),\n",
    "                                }, ensure_ascii=False) + '\\n')\n",
    "wp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "path = '/cognitive_comp/wutong/source/sim_data/raw_data_en/qqp_sim_data.json'\n",
    "feats = datasets.Features({\"text1\": datasets.Value('string'), \n",
    "                           \"text2\": datasets.Value('string'),\n",
    "                           \"score\": datasets.Value(\"int8\")\n",
    "                           })\n",
    "ds = (datasets.load_dataset('json', data_files=path, \n",
    "                            cache_dir='/cognitive_comp/wutong/source/data_base/huggingface-cache',\n",
    "                            features=feats)['train'])\n",
    "ds.save_to_disk('/cognitive_comp/wutong/source/sim_data/similarity_data_en/qqp_sim_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MRPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "wp = open('/cognitive_comp/wutong/source/sim_data/raw_data_en/MRPC/mrpc_dev.json', 'w')\n",
    "with open('/cognitive_comp/wutong/source/sim_data/raw_data_en/MRPC/msr_paraphrase_test.txt', 'r') as rp:\n",
    "    lines = rp.readlines()\n",
    "    for line in tqdm(lines):\n",
    "        line_list = line.split('\\t')\n",
    "        wp.write(json.dumps({'text1': str(line_list[-2]),\n",
    "                             'text2': str(line_list[-1]),\n",
    "                             'score': int(line_list[0][-1]),\n",
    "                            }, ensure_ascii=False) + '\\n')\n",
    "rp.close()\n",
    "wp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "\n",
    "path = '/cognitive_comp/wutong/source/sim_data/raw_data_en/MRPC/mrpc_dev.json'\n",
    "feats = datasets.Features({\"text1\": datasets.Value('string'), \n",
    "                           \"text2\": datasets.Value('string'),\n",
    "                           \"score\": datasets.Value('int8'),\n",
    "                           })\n",
    "ds = (datasets.load_dataset('json', data_files=path, \n",
    "                            cache_dir='/cognitive_comp/wutong/source/data_base/huggingface-cache',\n",
    "                            features=feats)['train'])\n",
    "ds.save_to_disk('/cognitive_comp/wutong/source/sim_data/similarity_data_en/mrpc_dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets, json\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_mrpc = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/sim_train_data/mrpc_train_ds')\n",
    "# dev_mrpc = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/sim_test_data/mrpc')\n",
    "\n",
    "with open('/cognitive_comp/wutong/source/sim_data/similarity_data_en/mrpc_data.json', 'w') as wp:\n",
    "    for idx in tqdm(range(train_mrpc.num_rows)):\n",
    "        wp.write(json.dumps({'sentence': train_mrpc[idx]['text1']}, ensure_ascii=False) + '\\n')\n",
    "        wp.write(json.dumps({'sentence': train_mrpc[idx]['text2']}, ensure_ascii=False) + '\\n')\n",
    "        wp.flush()\n",
    "    # for idx in tqdm(range(dev_mrpc.num_rows)):\n",
    "    #     wp.write(json.dumps({'sentence': dev_mrpc[idx]['text1']}, ensure_ascii=False) + '\\n')\n",
    "    #     wp.write(json.dumps({'sentence': dev_mrpc[idx]['text2']}, ensure_ascii=False) + '\\n')\n",
    "    #     wp.flush()\n",
    "wp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ds = (datasets.load_dataset('json', data_files='/cognitive_comp/wutong/source/sim_data/similarity_data_en/mrpc_data.json',\n",
    "                            cache_dir='/cognitive_comp/wutong/source/data_base/huggingface-cache')['train'])\n",
    "ds.save_to_disk(os.path.join('/cognitive_comp/wutong/source/sim_data/predict_sentences/mrpc_sentence'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PAWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"paws-x\", \"en\")\n",
    "dataset.save_to_disk('/cognitive_comp/wutong/source/sim_data/raw_data_en/paws')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets, json\n",
    "from tqdm import tqdm\n",
    "\n",
    "paws = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/raw_data_en/paws')\n",
    "paws_train = datasets.concatenate_datasets([paws['train'], paws['validation']])\n",
    "paws_val = paws['test']\n",
    "\n",
    "with open('/cognitive_comp/wutong/source/sim_data/similarity_data_en/paws-x_train.json', 'w') as wp:\n",
    "    for idx in tqdm(range(paws_train.num_rows)):\n",
    "        wp.write(json.dumps({'text1': str(paws_train[idx]['sentence1']),\n",
    "                             'text2': str(paws_train[idx]['sentence2']),\n",
    "                             'score': int(paws_train[idx]['label']),\n",
    "                            }, ensure_ascii=False) + '\\n')\n",
    "wp.close()\n",
    "\n",
    "with open('/cognitive_comp/wutong/source/sim_data/similarity_data_en/paws-x_dev.json', 'w') as wp1:\n",
    "    for idx in tqdm(range(paws_val.num_rows)):\n",
    "        wp1.write(json.dumps({'text1': str(paws_val[idx]['sentence1']),\n",
    "                             'text2': str(paws_val[idx]['sentence2']),\n",
    "                             'score': int(paws_val[idx]['label']),\n",
    "                            }, ensure_ascii=False) + '\\n')\n",
    "wp1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/cognitive_comp/wutong/source/sim_data/similarity_data_en/paws-x_train.json'\n",
    "path = '/cognitive_comp/wutong/source/sim_data/similarity_data_en/paws-x_dev.json'\n",
    "feats = datasets.Features({\"text1\": datasets.Value('string'), \n",
    "                           \"text2\": datasets.Value('string'),\n",
    "                           \"score\": datasets.Value('int8'),\n",
    "                           })\n",
    "ds = (datasets.load_dataset('json', data_files=path, \n",
    "                            cache_dir='/cognitive_comp/wutong/source/data_base/huggingface-cache',\n",
    "                            features=feats)['train'])\n",
    "# ds.save_to_disk('/cognitive_comp/wutong/source/sim_data/sim_train_data/paws_train_ds')\n",
    "ds.save_to_disk('/cognitive_comp/wutong/source/sim_data/sim_test_data/paws')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets, json\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_paws = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/sim_train_data/paws_train_ds')\n",
    "dev_paws = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/sim_test_data/paws')\n",
    "\n",
    "with open('/cognitive_comp/wutong/source/sim_data/similarity_data_en/paws_data.json', 'w') as wp:\n",
    "    for idx in tqdm(range(train_paws.num_rows)):\n",
    "        wp.write(json.dumps({'sentence': train_paws[idx]['text1']}, ensure_ascii=False) + '\\n')\n",
    "        wp.write(json.dumps({'sentence': train_paws[idx]['text2']}, ensure_ascii=False) + '\\n')\n",
    "        wp.flush()\n",
    "    for idx in tqdm(range(dev_paws.num_rows)):\n",
    "        wp.write(json.dumps({'sentence': dev_paws[idx]['text1']}, ensure_ascii=False) + '\\n')\n",
    "        wp.write(json.dumps({'sentence': dev_paws[idx]['text2']}, ensure_ascii=False) + '\\n')\n",
    "        wp.flush()\n",
    "wp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ds = (datasets.load_dataset('json', data_files='/cognitive_comp/wutong/source/sim_data/similarity_data_en/paws_data.json',\n",
    "                            cache_dir='/cognitive_comp/wutong/source/data_base/huggingface-cache')['train'])\n",
    "ds.save_to_disk(os.path.join('/cognitive_comp/wutong/source/sim_data/predict_sentences/paws_sentence'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets, json\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_paws = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/sim_train_data/paws_train_ds')\n",
    "dev_paws = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/sim_test_data/paws')\n",
    "paws = datasets.concatenate_datasets([train_paws, dev_paws])\n",
    "\n",
    "with open('/cognitive_comp/wutong/source/sim_data/raw_data_en/paws_sim_data.json', 'w') as wp:\n",
    "    for idx in tqdm(range(paws.num_rows)):\n",
    "        if paws[idx]['score'] == 1 and str(paws[idx]['text1']) != \"\" and str(paws[idx]['text2']) != \"\":\n",
    "            wp.write(json.dumps({'text1': str(paws[idx]['text1']),\n",
    "                                 'text2': str(paws[idx]['text2']),\n",
    "                                }, ensure_ascii=False) + '\\n')\n",
    "wp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "\n",
    "path = '/cognitive_comp/wutong/source/sim_data/raw_data_en/paws_sim_data.json'\n",
    "feats = datasets.Features({\"text1\": datasets.Value('string'), \n",
    "                           \"text2\": datasets.Value('string'),\n",
    "                           })\n",
    "ds = (datasets.load_dataset('json', data_files=path, \n",
    "                            cache_dir='/cognitive_comp/wutong/source/data_base/huggingface-cache',\n",
    "                            features=feats)['train'])\n",
    "ds.save_to_disk('/cognitive_comp/wutong/source/sim_data/similarity_data_en/paws_sim_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STS-B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "wp = open('/cognitive_comp/wutong/source/sim_data/raw_data_en/STS-B/sts-b.json', 'w')\n",
    "with open('/cognitive_comp/wutong/source/sim_data/raw_data_en/STS-B/train.tsv', 'r') as rp:\n",
    "    lines = rp.readlines()\n",
    "    for line in tqdm(lines):\n",
    "        line_list = line.split('\\t')\n",
    "        if float(line_list[-1][:-1]) >= 4.0:\n",
    "            wp.write(json.dumps({'text1': str(line_list[-3]),\n",
    "                                'text2': str(line_list[-2]),\n",
    "                                }, ensure_ascii=False) + '\\n')\n",
    "with open('/cognitive_comp/wutong/source/sim_data/raw_data_en/STS-B/dev.tsv', 'r') as rp:\n",
    "    lines = rp.readlines()\n",
    "    for line in tqdm(lines):\n",
    "        line_list = line.split('\\t')\n",
    "        if float(line_list[-1][:-1]) >= 4.0:\n",
    "            wp.write(json.dumps({'text1': str(line_list[-3]),\n",
    "                                'text2': str(line_list[-2]),\n",
    "                                }, ensure_ascii=False) + '\\n')\n",
    "rp.close()\n",
    "wp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "\n",
    "path = '/cognitive_comp/wutong/source/sim_data/raw_data_en/STS-B/sts-b.json'\n",
    "feats = datasets.Features({\"text1\": datasets.Value('string'), \n",
    "                           \"text2\": datasets.Value('string'),\n",
    "                           })\n",
    "ds = (datasets.load_dataset('json', data_files=path, \n",
    "                            cache_dir='/cognitive_comp/wutong/source/data_base/huggingface-cache',\n",
    "                            features=feats)['train'])\n",
    "ds.save_to_disk('/cognitive_comp/wutong/source/sim_data/similarity_data_en/sts-b_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WikiText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "\n",
    "wikitext = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/raw_data_en/wikitext')\n",
    "wikitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "with open('/cognitive_comp/wutong/source/sim_data/raw_data_en/wikitext.json', 'w') as wp:\n",
    "    for idx in tqdm(range(wikitext.num_rows)):\n",
    "        if len(wikitext[idx]['text']) != 0 and '=' not in wikitext[idx]['text']:\n",
    "            wp.write(json.dumps({'text1': str(wikitext[idx]['text']),\n",
    "                                 'text2': 'general',\n",
    "                                }, ensure_ascii=False) + '\\n')\n",
    "wp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "\n",
    "path = '/cognitive_comp/wutong/source/sim_data/raw_data_en/wikitext.json'\n",
    "feats = datasets.Features({\"text1\": datasets.Value('string'), \n",
    "                           \"text2\": datasets.Value('string'),\n",
    "                           })\n",
    "ds = (datasets.load_dataset('json', data_files=path, \n",
    "                            cache_dir='/cognitive_comp/wutong/source/data_base/huggingface-cache',\n",
    "                            features=feats)['train'])\n",
    "ds.save_to_disk('/cognitive_comp/wutong/source/sim_data/similarity_data_en/wikitext_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets, json\n",
    "from tqdm import tqdm\n",
    "\n",
    "wiki_data = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/similarity_data_en/wikitext_data')\n",
    "with open('/cognitive_comp/wutong/source/sim_data/raw_data_en/wikitext.json', 'w') as wp:\n",
    "    for idx in tqdm(range(wiki_data.num_rows)):\n",
    "        if len(wiki_data[idx]['text1']) >= 50 and '<unk>' not in wiki_data[idx]['text1'] and '@' not in wiki_data[idx]['text1']:\n",
    "            wp.write(json.dumps({'text1': str(wiki_data[idx]['text1']),\n",
    "                                 'text2': 'general',\n",
    "                                }, ensure_ascii=False) + '\\n')\n",
    "wp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/cognitive_comp/wutong/source/sim_data/raw_data_en/wikitext.json'\n",
    "feats = datasets.Features({\"text1\": datasets.Value('string'), \n",
    "                           \"text2\": datasets.Value('string'),\n",
    "                           })\n",
    "ds = (datasets.load_dataset('json', data_files=path, \n",
    "                            cache_dir='/cognitive_comp/wutong/source/data_base/huggingface-cache',\n",
    "                            features=feats)['train'])\n",
    "ds.save_to_disk('/cognitive_comp/wutong/source/sim_data/similarity_data_en/wikitext_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 合并数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "wikitext = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/similarity_data_en/wikitext_data')\n",
    "qqp = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/similarity_data_en/qqp_sim_data')\n",
    "sts_b = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/similarity_data_en/sts-b_data')\n",
    "paws = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/similarity_data_en/paws_sim_data')\n",
    "\n",
    "pretrain_data = datasets.concatenate_datasets([wikitext, qqp, sts_b, paws])\n",
    "pretrain_data = pretrain_data.shuffle(seed=42)\n",
    "pretrain_data.save_to_disk('/cognitive_comp/wutong/source/sim_data/similarity_data_en/pretrain_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "\n",
    "pretrain_data = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/similarity_data_en/pretrain_data')\n",
    "pretrain_data = pretrain_data.train_test_split(test_size=0.03)\n",
    "pretrain_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_data['train'].save_to_disk('/cognitive_comp/wutong/source/sim_data/similarity_data_en/pre_train')\n",
    "pretrain_data['test'].save_to_disk('/cognitive_comp/wutong/source/sim_data/similarity_data_en/pre_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "qqp = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/similarity_data_en/qqp_sim_data')\n",
    "\n",
    "# train_paws = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/sim_train_data/paws_train_ds')\n",
    "# dev_paws = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/sim_test_data/paws')\n",
    "\n",
    "train_mrpc = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/sim_train_data/mrpc_train_ds')\n",
    "dev_mrpc = datasets.load_from_disk('/cognitive_comp/wutong/source/sim_data/sim_test_data/mrpc')\n",
    "\n",
    "data4paws = datasets.concatenate_datasets([qqp, train_mrpc, dev_mrpc])\n",
    "data4paws = data4paws.shuffle(seed=42)\n",
    "data4paws.save_to_disk('/cognitive_comp/wutong/source/sim_data/similarity_data_en/labeled4paws')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4paws = data4paws.train_test_split(test_size=0.03)\n",
    "data4paws['train'].save_to_disk('/cognitive_comp/wutong/source/sim_data/similarity_data_en/labeled_train_paws')\n",
    "data4paws['test'].save_to_disk('/cognitive_comp/wutong/source/sim_data/similarity_data_en/labeled_test_paws')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试生成结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "from transformers import OPTForCausalLM, GPT2Tokenizer\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "model = OPTForCausalLM.from_pretrained('/cognitive_comp/wutong/source/model_base/model_en/opt-2.7b')\n",
    "state_dict = torch.load('/cognitive_comp/wutong/source/model_base/model_en/opt-2.7b.pt',\n",
    "                        map_location='cpu')['module']\n",
    "new_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    if any([i in k for i in ['module.generator.gen.']]):\n",
    "        new_dict[k[len('module.generator.gen.'):]] = v\n",
    "    else:\n",
    "        continue\n",
    "if new_dict == {}:\n",
    "    new_dict = state_dict\n",
    "model.load_state_dict(new_dict)\n",
    "model.to('cuda')\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('/cognitive_comp/wutong/source/model_base/model_en/opt-2.7b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"\\\"In Paris , in October 1560 , he secretly met the English ambassador , Nicolas Throckmorton , asking him for a passport to return to England through Scotland .\\\" is similar to \\\"\", \n",
    "          \"\\\"The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .\\\" is similar to \\\"\"]\n",
    "inputs = tokenizer.batch_encode_plus(prompt, return_tensors=\"pt\", padding=True)\n",
    "# Generate\n",
    "generate_ids = model.generate(inputs.input_ids.cuda(), do_sample=True, top_p=0.8, max_length=200, num_return_sequences=1)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 杂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 相似句数据增广"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets, json\n",
    "from tqdm import tqdm\n",
    "\n",
    "wp = open('/cognitive_comp/wutong/bustm.json', 'w')\n",
    "with open('/cognitive_comp/wutong/train_0.json', 'r') as rp:\n",
    "    lines = rp.readlines()\n",
    "    for line in tqdm(lines):\n",
    "        data = json.loads(line)\n",
    "        wp.write(json.dumps({'sentence': data['sentence1']}, ensure_ascii=False) + '\\n')\n",
    "        wp.write(json.dumps({'sentence': data['sentence2']}, ensure_ascii=False) + '\\n')\n",
    "        wp.flush()\n",
    "\n",
    "rp.close()\n",
    "wp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/cognitive_comp/wutong/bustm.json'\n",
    "feats = datasets.Features({\"sentence\": datasets.Value('string')})\n",
    "ds = (datasets.load_dataset('json', data_files=path, \n",
    "                            cache_dir='/cognitive_comp/wutong/source/data_base/huggingface-cache',\n",
    "                            features=feats)['train'])\n",
    "ds.save_to_disk('/cognitive_comp/wutong/bustm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def gen_pred_collate(batch_data, gen_tokenizer):\n",
    "    input_ids, length_list = [], []\n",
    "    for item in batch_data:\n",
    "        if not item['text2']:\n",
    "            continue\n",
    "        cur_input_ids = gen_tokenizer(\n",
    "            '<bos>“' + item['text2'] + '”的相似句是“', return_tensors='pt'\n",
    "        ).input_ids.squeeze()[:-1]  # 不能加<eos>\n",
    "\n",
    "        # 每个样本复制 N 份\n",
    "        length = [cur_input_ids.size(0)] * 2\n",
    "        cur_input_ids = [cur_input_ids] * 2\n",
    "\n",
    "        length_list.extend(length)\n",
    "        input_ids.extend(cur_input_ids)\n",
    "\n",
    "    input_ids = pad_sequence(\n",
    "        [x for x in input_ids], batch_first=True, \n",
    "        padding_value=gen_tokenizer.pad_token_id)\n",
    "    length_tensor = torch.tensor(length_list)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'length_tensor': length_tensor,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys, glob, datasets\n",
    "sys.path.append('/cognitive_comp/wutong/similarity_generation/')\n",
    "from data_utlis.sim_gen_dataset import SimGanDataset\n",
    "\n",
    "for k in range(1, 21):\n",
    "    num = k\n",
    "    data = datasets.load_from_disk(f'/cognitive_comp/wutong/bustm{num-1}')\n",
    "\n",
    "    gen_tokenizer = T5Tokenizer.from_pretrained(\n",
    "        '/cognitive_comp/wutong/source/model_base/chinese_sentencepiece/cog-pretrain.model',\n",
    "        eos_token='<|endoftext|>',\n",
    "        pad_token='<|endoftext|>',\n",
    "        extra_ids=0)\n",
    "    gen_tokenizer.add_special_tokens({'bos_token': '<bos>'})\n",
    "\n",
    "    predict_dataset = SimGanDataset(data)\n",
    "    def collate_fn(batch_data):\n",
    "        return gen_pred_collate(batch_data, gen_tokenizer)\n",
    "    dataloader = DataLoader(\n",
    "        dataset=predict_dataset,\n",
    "        batch_size=128,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    import numpy as np\n",
    "    import sys, evaluate, torch, json\n",
    "    sys.path.append('/cognitive_comp/wutong/similarity_generation/')\n",
    "    from data_utlis.sample_sequence import sample_sequence_batch\n",
    "    from model_utils.sim_gen_model import Generator\n",
    "\n",
    "    import os\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '4'\n",
    "\n",
    "    class Config:\n",
    "        cycle = 0\n",
    "        chinese = 1\n",
    "        txl_config_path = '/cognitive_comp/wutong/similarity_generation/model_utils/txl_5B_config.json'\n",
    "        txl_model_path = '/cognitive_comp/wutong/source/model_base/model_zh/txl_zh_5.0B.pt'\n",
    "        ckpt_model_path = '/cognitive_comp/wutong/similarity_generation/all_checkpoints/chip0'\n",
    "\n",
    "    config = Config()\n",
    "    perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "    raw_texts, sim_texts, mean_perplexity = [], [], []\n",
    "    for idx in [10]:\n",
    "        config.cycle = idx\n",
    "        generator = Generator(config)\n",
    "        generator.half().eval().cuda()\n",
    "        sim_sent_list = []\n",
    "        for batch in dataloader:\n",
    "            torch.cuda.empty_cache()\n",
    "            output_dict = sample_sequence_batch(\n",
    "                model=generator.gen, context_tokens_tensor=batch['input_ids'].cuda(),\n",
    "                context_length_tensor=batch['length_tensor'], repetition_penalty=1.0, max_out_seq=200,\n",
    "                end_token_id=50000, temperature=1.0, top_k=0, top_p=0.5,\n",
    "            )\n",
    "\n",
    "            sim_sent_list.extend(\n",
    "                gen_tokenizer.batch_decode(output_dict['ids_list'], skip_special_tokens=True))\n",
    "\n",
    "        with open(f'/cognitive_comp/wutong/bustm{num}.json', 'w') as wp:\n",
    "            for jdx, item in tqdm(enumerate(sim_sent_list)):\n",
    "                item = item.replace(' ', '').split('”的相似句是“')\n",
    "                if len(item) == 2 and item[0][1:] != item[1][:-1]:\n",
    "                    wp.write(json.dumps({'text1': item[0][1:],\n",
    "                                        'text2': item[1][:-1]}, ensure_ascii=False) + '\\n')\n",
    "            wp.close()\n",
    "        \n",
    "        path = f'/cognitive_comp/wutong/bustm{num}.json'\n",
    "        feats = datasets.Features({\"text1\": datasets.Value('string'), \n",
    "                                \"text2\": datasets.Value('string'),\n",
    "                                })\n",
    "        ds = (datasets.load_dataset('json', data_files=path, \n",
    "                                    cache_dir='/cognitive_comp/wutong/source/data_base/huggingface-cache',\n",
    "                                    features=feats)['train'])\n",
    "        ds.save_to_disk(f'/cognitive_comp/wutong/bustm{num}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for i in range(8):\n",
    "    data_list.append(datasets.load_from_disk('/cognitive_comp/wutong/bustm' + str(i)))\n",
    "bustm = datasets.concatenate_datasets(data_list)\n",
    "bustm.save_to_disk('/cognitive_comp/wutong/bustm0_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'/cognitive_comp/wutong/gen_data.json', 'w') as wp:\n",
    "    for i in tqdm(range(bustm.num_rows)):\n",
    "        item = bustm[i]\n",
    "        if item['text1'] and item['text2']:\n",
    "            wp.write(json.dumps({'sentence1': item['text1'],\n",
    "                                 'sentence2': item['text2']}, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bert Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets, os\n",
    "from bert_score import score\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "for i in [0, 8]:  # [0, 10]\n",
    "    raw_text, sim_text = [], []\n",
    "    data = datasets.load_from_disk('/cognitive_comp/wutong/similarity_generation/consistency/afqmc/data_cycle_' + str(i))\n",
    "    for j in range(data.num_rows):\n",
    "        raw_text.append(data[j]['text1'])\n",
    "        sim_text.append(data[j]['text2'])\n",
    "    P, R, F1 = score(sim_text, raw_text, lang=\"zh\", verbose=True)\n",
    "    print(F1.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets, os, evaluate\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "\n",
    "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "for i in [0, 8]:\n",
    "    ppl, sim_text = [], []\n",
    "    data = datasets.load_from_disk('/cognitive_comp/wutong/similarity_generation/consistency/mrpc/data_cycle_' + str(i))\n",
    "    for j in tqdm(range(data.num_rows)):\n",
    "        if data[j]['text2']:\n",
    "            sim_text.append(data[j]['text2'])\n",
    "    print(perplexity.compute(input_texts=sim_text, model_id='gpt2')['mean_perplexity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import GPT2Tokenizer,GPT2LMHeadModel\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('IDEA-CCNL/Wenzhong-GPT2-110M')\n",
    "model = GPT2LMHeadModel.from_pretrained('IDEA-CCNL/Wenzhong-GPT2-110M')\n",
    "model.to('cuda').eval()\n",
    "\n",
    "def gpt_ppl(sent):\n",
    "    inputs = tokenizer(sent, return_tensors='pt')\n",
    "    loss = model(input_ids=inputs[\"input_ids\"].cuda(), \n",
    "                 attention_mask=inputs[\"attention_mask\"].cuda(),\n",
    "                 labels=inputs[\"input_ids\"].cuda()).loss\n",
    "    ppl = np.exp(loss.item())\n",
    "\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "for i in [0, 10]:\n",
    "    ppl, sim_text = [], []\n",
    "    data = datasets.load_from_disk('/cognitive_comp/wutong/similarity_generation/consistency/qqp/data_cycle_' + str(i))\n",
    "    for j in tqdm(range(data.num_rows)):\n",
    "        if data[j]['text2']:\n",
    "            ppl.append(gpt_ppl(data[j]['text2']))\n",
    "    print(np.array(ppl).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\")  # princeton-nlp/sup-simcse-bert-base-uncased / SimCSE-bert-base\n",
    "model = BertModel.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\")\n",
    "model.to('cuda').eval()\n",
    "\n",
    "def get_emb(sent_list):\n",
    "    torch.cuda.empty_cache()\n",
    "    inputs = tokenizer(sent_list, padding=True, return_tensors=\"pt\")\n",
    "    outputs = model(input_ids=inputs[\"input_ids\"].cuda(), \n",
    "                    attention_mask=inputs[\"attention_mask\"].cuda()\n",
    "                ).pooler_output \n",
    "\n",
    "    return outputs.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import wasserstein_distance, entropy\n",
    "\n",
    "\n",
    "all_w, all_e, all_kl, all_h = [], [], [], []\n",
    "for i in range(9):  # [0, 10]\n",
    "    sim_text, raw_text = [], []\n",
    "    cos_sim, prob = [], []\n",
    "    data = datasets.load_from_disk('/cognitive_comp/wutong/similarity_generation/consistency/mrpc/data_cycle_' + str(i))\n",
    "    for j in tqdm(range(data.num_rows)):\n",
    "        if data[j]['text1'] and data[j]['text2']:\n",
    "            prob.append(data[j]['prob'])\n",
    "            raw_text.append(data[j]['text1'])\n",
    "            sim_text.append(data[j]['text2'])\n",
    "        \n",
    "        if j != 0 and (j % 100 == 0 or j == data.num_rows - 1):\n",
    "            emb1 = get_emb(raw_text).tolist()\n",
    "            emb2 = get_emb(sim_text).tolist()\n",
    "            for e1, e2 in zip(emb1, emb2):\n",
    "                e1, e2 = np.array(e1), np.array(e2)\n",
    "                cos_sim.append(e1.dot(e2) / (np.linalg.norm(e1) * np.linalg.norm(e2)))\n",
    "            sim_text, raw_text = [], []\n",
    "    \n",
    "    del_index = []\n",
    "    for j in range(len(cos_sim)):\n",
    "        if np.isnan(cos_sim[j]) or cos_sim[j] <= 0:\n",
    "            del_index.append(j)\n",
    "    for j in reversed(del_index):\n",
    "        del prob[j]\n",
    "        del cos_sim[j]\n",
    "    \n",
    "    print(len(cos_sim))\n",
    "    \n",
    "    # e_d = np.sqrt(np.sum(np.square(np.array(cos_sim) - np.array(prob))))\n",
    "    # all_e.append(e_d)\n",
    "\n",
    "    kl = entropy(prob, cos_sim)\n",
    "    all_kl.append(kl)\n",
    "\n",
    "    # w_d = wasserstein_distance(cos_sim, prob)\n",
    "    # all_w.append(w_d)\n",
    "\n",
    "    # h_d = 1 / np.sqrt(2) * np.linalg.norm(np.sqrt(cos_sim) - np.sqrt(prob))\n",
    "    # all_h.append(h_d)\n",
    "    \n",
    "    # print(all_e)\n",
    "    print(all_kl)\n",
    "    # print(all_w)\n",
    "    # print(all_h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "pt_path = '/cognitive_comp/wutong/source/model_base/pretrained_zh/ernie_base_mc'\n",
    "dis_tokenizer = AutoTokenizer.from_pretrained(pt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = dis_tokenizer.encode_plus('hello:'*500, max_length=512, padding=\"longest\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "wp = open('/cognitive_comp/wutong/source/sim_data/raw_data/bustm/new_unlabel.json', 'w')\n",
    "\n",
    "with open('/cognitive_comp/wutong/source/sim_data/raw_data/bustm/unlabeled.json', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in tqdm(lines):\n",
    "        line = eval(line)\n",
    "        wp.write(json.dumps({'sentence': line['sentence1']}, ensure_ascii=False) + '\\n')\n",
    "        wp.write(json.dumps({'sentence': line['sentence2']}, ensure_ascii=False) + '\\n')\n",
    "    f.close()\n",
    "wp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, datasets\n",
    "\n",
    "test_ds = datasets.Dataset.from_json('/cognitive_comp/wutong/source/sim_data/raw_data/bustm/new_unlabel.json')\n",
    "\n",
    "random_list = random.sample(range(60), 10)\n",
    "data = test_ds.select(random_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(test_ds[random_list[i]])\n",
    "    print(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2579772665e14a92e42a466d49ec9ff85683bc5df8d0c675aa9afdde4fd8e604"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
